{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "import jdatetime\n",
    "from pyspark import Row, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "from c2cbet import config\n",
    "from pyspark.sql import functions\n",
    "from dputils import pyspark_funcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "spark = SparkSession \\\n",
    "    .builder.appName(config.appName) \\\n",
    "    .master(config.master) \\\n",
    "    .config(\"spark.sql.warehouse.dir\", config.spark_sql_warehouse_dir, SparkConf()) \\\n",
    "    .getOrCreate()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- firstname: string (nullable = true)\n",
      " |-- lastname: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- salary: long (nullable = true)\n",
      " |-- creationDate: long (nullable = true)\n",
      "\n",
      "+---------+--------+------+------+------------+\n",
      "|firstname|lastname|gender|salary|creationDate|\n",
      "+---------+--------+------+------+------------+\n",
      "|    James|   Smith|     M|  3000|  1646822155|\n",
      "|     Anna|    Rose|     F|  4100|  1646722155|\n",
      "|   Robert|Williams|     M|  6200|  1646622155|\n",
      "+---------+--------+------+------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# df = spark.createDataFrame(\n",
    "#     [(1, \"foo\"),  # create your data here, be consistent in the types.\n",
    "#      (2, \"bar\"),],\n",
    "#     [\"id\", \"label\"]  # add your column names here\n",
    "# )\n",
    "data = [('James','Smith','M',3000,1646822155), ('Anna','Rose','F',4100,1646722155),\n",
    "        ('Robert','Williams','M',6200,1646622155)\n",
    "        ]\n",
    "columns = [\"firstname\",\"lastname\",\"gender\",\"salary\",\"creationDate\"]\n",
    "df = spark.createDataFrame(data=data, schema = columns)\n",
    "\n",
    "df.printSchema()\n",
    "df.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Add new constanct column\n",
      "+---------+--------+------+------+------------+-------------+\n",
      "|firstname|lastname|gender|salary|creationDate|bonus_percent|\n",
      "+---------+--------+------+------+------------+-------------+\n",
      "|    James|   Smith|     M|  3000|  1646822155|          0.3|\n",
      "|     Anna|    Rose|     F|  4100|  1646722155|          0.3|\n",
      "|   Robert|Williams|     M|  6200|  1646622155|          0.3|\n",
      "+---------+--------+------+------+------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#How to add a constant column in a Spark DataFrame?\n",
    "# In this PySpark article, I will explain different ways of how to add a new column to DataFrame\n",
    "# using withColumn(), select(), sql(), Few ways include adding a constant column with a default value,\n",
    "# derive based out of another column, add a column with NULL/None value, add multiple columns\n",
    "\n",
    "print(\">>> Add new constanct column\")\n",
    "from pyspark.sql.functions import lit\n",
    "df.withColumn(\"bonus_percent\", lit(0.3)) \\\n",
    "    .show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Add New column with NULL\n",
      "+---------+--------+------+------+------------+-----------+\n",
      "|firstname|lastname|gender|salary|creationDate|DEFAULT_COL|\n",
      "+---------+--------+------+------+------------+-----------+\n",
      "|    James|   Smith|     M|  3000|  1646822155|       null|\n",
      "|     Anna|    Rose|     F|  4100|  1646722155|       null|\n",
      "|   Robert|Williams|     M|  6200|  1646622155|       null|\n",
      "+---------+--------+------+------+------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\">>> Add New column with NULL\")\n",
    "df.withColumn(\"DEFAULT_COL\", lit(None)) \\\n",
    "    .show()\n",
    "\n",
    "# def to_jdate()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Add column from existing column\n",
      "+---------+--------+------+------+------------+------------+-------------------+-------------------+\n",
      "|firstname|lastname|gender|salary|creationDate|bonus_amount|     creationDate_d|     creationDate_j|\n",
      "+---------+--------+------+------+------------+------------+-------------------+-------------------+\n",
      "|    James|   Smith|     M|  3000|  1646822155|       900.0|2022-03-09 14:05:55|1400-12-18 14:05:55|\n",
      "|     Anna|    Rose|     F|  4100|  1646722155|      1230.0|2022-03-08 10:19:15|1400-12-17 10:19:15|\n",
      "|   Robert|Williams|     M|  6200|  1646622155|      1860.0|2022-03-07 06:32:35|1400-12-16 06:32:35|\n",
      "+---------+--------+------+------+------------+------------+-------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\">>> Add column from existing column\")\n",
    "# jfrom_unixtime=functions.udf(lambda ts: jdatetime.datetime.fromtimestamp(ts).strftime('%Y-%m-%d %H:%M:%S') , StringType() )\n",
    "df.withColumn(\"bonus_amount\", df.salary*0.3) \\\n",
    "    .withColumn(\"creationDate_d\", functions.from_unixtime(df.creationDate)) \\\n",
    "    .withColumn(\"creationDate_j\", pyspark_funcs.jfrom_unixtime(df.creationDate)) \\\n",
    "    .show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(\">>> Add column by concatinating existing columns\")\n",
    "from pyspark.sql.functions import concat_ws\n",
    "df.withColumn(\"name\", concat_ws(\",\",\"firstname\",'lastname')) \\\n",
    "    .show()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(\">>> Add Column Value Based on Condition\")\n",
    "from pyspark.sql.functions import when\n",
    "df.withColumn(\"grade\",\n",
    "              when((df.salary < 4000), lit(\"A\"))\n",
    "              .when((df.salary >= 4000) & (df.salary <= 5000), lit(\"B\"))\n",
    "              .otherwise(lit(\"C\"))\n",
    "              ).show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(\">>> Add Column When not Exists on DataFrame\")\n",
    "if 'dummy' not in df.columns:\n",
    "    df.withColumn(\"dummy\",lit(None)) \\\n",
    "        .show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(\">>>  Add Multiple Columns using Map\")\n",
    "# Let's assume DF has just 3 columns c1,c2,c3\n",
    "# apply transformation on these columns and derive multiple columns\n",
    "# and store these column vlaues into c5,c6,c7,c8,c9,10\n",
    "\n",
    "# df2 = df.rdd.map(row=>{(c1,c2,c5,c6,c7,c8,c9,c10)})"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "Table or view not found: c2c_bet; line 1 pos 29;\n'Aggregate ['channel], ['channel, unresolvedalias(count(1), None)]\n+- 'UnresolvedRelation [c2c_bet], [], false\n",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mAnalysisException\u001B[0m                         Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp\\ipykernel_29240\\158380129.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[1;32m----> 1\u001B[1;33m \u001B[0mspark\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msql\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m\"select channel,count(*) from c2c_bet group by channel\"\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mshow\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      2\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      3\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\app\\python38\\lib\\site-packages\\pyspark\\sql\\session.py\u001B[0m in \u001B[0;36msql\u001B[1;34m(self, sqlQuery)\u001B[0m\n\u001B[0;32m    721\u001B[0m         \u001B[1;33m[\u001B[0m\u001B[0mRow\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mf1\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mf2\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;34m'row1'\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mRow\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mf1\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;36m2\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mf2\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;34m'row2'\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mRow\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mf1\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;36m3\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mf2\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;34m'row3'\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    722\u001B[0m         \"\"\"\n\u001B[1;32m--> 723\u001B[1;33m         \u001B[1;32mreturn\u001B[0m \u001B[0mDataFrame\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_jsparkSession\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msql\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0msqlQuery\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_wrapped\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    724\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    725\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0mtable\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtableName\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\app\\python38\\lib\\site-packages\\py4j\\java_gateway.py\u001B[0m in \u001B[0;36m__call__\u001B[1;34m(self, *args)\u001B[0m\n\u001B[0;32m   1319\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1320\u001B[0m         \u001B[0manswer\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mgateway_client\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msend_command\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mcommand\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1321\u001B[1;33m         return_value = get_return_value(\n\u001B[0m\u001B[0;32m   1322\u001B[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001B[0;32m   1323\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\app\\python38\\lib\\site-packages\\pyspark\\sql\\utils.py\u001B[0m in \u001B[0;36mdeco\u001B[1;34m(*a, **kw)\u001B[0m\n\u001B[0;32m    115\u001B[0m                 \u001B[1;31m# Hide where the exception came from that shows a non-Pythonic\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    116\u001B[0m                 \u001B[1;31m# JVM exception message.\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 117\u001B[1;33m                 \u001B[1;32mraise\u001B[0m \u001B[0mconverted\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    118\u001B[0m             \u001B[1;32melse\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    119\u001B[0m                 \u001B[1;32mraise\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mAnalysisException\u001B[0m: Table or view not found: c2c_bet; line 1 pos 29;\n'Aggregate ['channel], ['channel, unresolvedalias(count(1), None)]\n+- 'UnresolvedRelation [c2c_bet], [], false\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select channel,count(*) from c2c_bet group by channel\").show()\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}