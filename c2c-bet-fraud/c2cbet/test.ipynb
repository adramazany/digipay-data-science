{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "from c2cbet import config, hlpr\n",
    "from c2cbet.helper import spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+----------------+-------------------+--------------------+--------------------+--------------------+------+---------------+-------------+---------+--------------+-------------+----------------+-------------------+------------------+---------------------+---------------------+------------+-------------------+--------------------+--------------------+--------------------+--------------+--------------------+----------------+--------------------+--------------+--------------------+\n",
      "|                 AID|        OWNER_USERID|OWNER_CELLNUMBER|             AMOUNT|        CREATIONDATE|      EXPIRATIONDATE|        EXERCISEDATE|STATUS|      FEECHARGE|         TYPE|INITIATOR|SOURCE_POSTFIX|SOURCE_PREFIX|SOURCE_BANK_CODE|DESTINATION_POSTFIX|DESTINATION_PREFIX|DESTINATION_BANK_CODE|DESTINATION_CARDINDEX|         RRN|              GDATE|                  IP|        TRACKINGCODE|      creationdate_d|creationdate_j|    expirationdate_d|expirationdate_j|      exercisedate_d|exercisedate_j|diff_exercise_create|\n",
      "+--------------------+--------------------+----------------+-------------------+--------------------+--------------------+--------------------+------+---------------+-------------+---------+--------------+-------------+----------------+-------------------+------------------+---------------------+---------------------+------------+-------------------+--------------------+--------------------+--------------------+--------------+--------------------+----------------+--------------------+--------------+--------------------+\n",
      "|60565c55629149000...|7395dd6c-cae3-441...|     09126494166|13000000.0000000000|1616272469385.000...|1616358869385.000...|1616272474997.000...|     8|8400.0000000000|CARD_TRANSFER|   MOBILE|          6195|       504706|             061|               9355|            603770|                  016|                 null|        null|2021-03-21 00:00:00|2.147.59.5, 2.147...|16968857901616272...|+53187-08-21 11:1...|          null|+53190-05-17 11:1...|            null|+53187-08-21 12:5...|          null|      5612.000000000|\n",
      "|60565cc084a54c000...|6bacd64a-d9ab-499...|     09308118835| 2700000.0000000000|1616272576818.000...|1616358976818.000...|1616272578914.000...|     8|          0E-10|CARD_TRANSFER|   MOBILE|          1496|       622106|             054|               9036|            622106|                  054|                 null|       61351|2021-03-21 00:00:00|151.234.14.137, 1...|77383579016162725...|+53187-08-22 17:1...|          null|+53190-05-18 17:1...|            null|+53187-08-22 17:4...|          null|      2096.000000000|\n",
      "|60565cc57b3387000...|fe591dcd-2bac-424...|     09125607421| 2500000.0000000000|1616272581342.000...|1616358981342.000...|1616272584558.000...|     8|6000.0000000000|CARD_TRANSFER|   MOBILE|          5929|       585983|             018|               7305|            610433|                  012|                 null|        null|2021-03-21 00:00:00|89.38.102.5, 89.3...|59324525616162725...|+53187-08-22 18:2...|          null|+53190-05-18 18:2...|            null|+53187-08-22 19:1...|          null|      3216.000000000|\n",
      "|605772f8629149000...|b0837814-256d-4b5...|     09197298580| 2000000.0000000000|1616343800700.000...|1616430200700.000...|1616343804162.000...|     8|6000.0000000000|CARD_TRANSFER|   MOBILE|          1156|       502229|             057|               0622|            610433|                  012|                 null| 27309890255|2021-03-21 00:00:00|86.55.161.87, 86....|16790535681616343...|+53189-11-24 00:3...|          null|+53192-08-20 01:3...|            null|+53189-11-24 01:3...|          null|      3462.000000000|\n",
      "|605772f9629149000...|9a4f5456-84d6-484...|     09172703585|  460200.0000000000|1616343801497.000...|1616430201497.000...|1616343804527.000...|     8|6000.0000000000|CARD_TRANSFER|   MOBILE|          8528|       585983|             018|               1778|            627381|                  063|                 null|        null|2021-03-21 00:00:00|5.134.149.54, 5.1...|19614581611616343...|+53189-11-24 00:4...|          null|+53192-08-20 01:4...|            null|+53189-11-24 01:3...|          null|      3030.000000000|\n",
      "|605773007b3387000...|266c620a-5ded-411...|     09028589356| 5000000.0000000000|1616343808764.000...|1616430208764.000...|1616343810601.000...|     8|6000.0000000000|CARD_TRANSFER|   MOBILE|          9266|       610433|             012|               4203|            628023|                  014|                 null|107843220774|2021-03-21 00:00:00|95.162.155.10, 95...|11586064591616343...|+53189-11-24 02:4...|          null|+53192-08-20 03:4...|            null|+53189-11-24 03:2...|          null|      1837.000000000|\n",
      "|60577305629149000...|4b09ff23-805b-42e...|     09373737339| 3800000.0000000000|1616343813031.000...|1616430213031.000...|1616343815543.000...|     8|6000.0000000000|CARD_TRANSFER|   MOBILE|          6637|       622106|             054|               2347|            603799|                  017|                 null|      445351|2021-03-21 00:00:00|89.45.126.144, 89...|14870134591616343...|+53189-11-24 04:0...|          null|+53192-08-20 05:0...|            null|+53189-11-24 04:4...|          null|      2512.000000000|\n",
      "|60565d047b3387000...|ad7ab34e-e00c-42e...|     09195560293|10000000.0000000000|1616272644724.000...|1616359044724.000...|1616272651339.000...|     8|6000.0000000000|CARD_TRANSFER|   MOBILE|          4952|       504706|             061|               7459|            628023|                  014|                 null|        null|2021-03-21 00:00:00|89.196.73.168, 89...|65216752016162726...|+53187-08-23 12:0...|          null|+53190-05-19 12:0...|            null|+53187-08-23 13:5...|          null|      6615.000000000|\n",
      "|60565d10629149000...|4ba2af4a-a658-41c...|     09187109687| 2600000.0000000000|1616272656581.000...|1616359056581.000...|1616272662507.000...|     8|6000.0000000000|CARD_TRANSFER|   MOBILE|          6889|       636949|             065|               2629|            610433|                  012|                 null|108000745003|2021-03-21 00:00:00|5.124.54.251, 5.1...|19861407191616272...|+53187-08-23 15:1...|          null|+53190-05-19 15:1...|            null|+53187-08-23 16:5...|          null|      5926.000000000|\n",
      "|60565d2f629149000...|6bacd64a-d9ab-499...|     09308118835| 3800000.0000000000|1616272687310.000...|1616359087310.000...|1616272689969.000...|     8|6000.0000000000|CARD_TRANSFER|   MOBILE|          1496|       622106|             054|               0024|            628023|                  014|                 null|       61367|2021-03-21 00:00:00|151.234.14.137, 1...|28768904916162726...|+53187-08-23 23:5...|          null|+53190-05-19 23:5...|            null|+53187-08-24 00:3...|          null|      2659.000000000|\n",
      "|60565d65629149000...|ea52ac93-76ea-48d...|     09910933197|10000000.0000000000|1616272741350.000...|1616359141350.000...|1616272745852.000...|     8|6000.0000000000|CARD_TRANSFER|   MOBILE|          9999|       627381|             063|               8868|            589210|                  015|                 null|130004998136|2021-03-21 00:00:00|83.120.67.197, 83...|11047406131616272...|+53187-08-24 14:5...|          null|+53190-05-20 14:5...|            null|+53187-08-24 16:0...|          null|      4502.000000000|\n",
      "|60565ddd629149000...|5ae65f46-ef52-4ae...|     09035093668|  900000.0000000000|1616272861613.000...|1616359261613.000...|1616272874169.000...|     8|6000.0000000000|CARD_TRANSFER|   MOBILE|          4819|       504706|             061|               9311|            589210|                  015|                 null|        null|2021-03-21 00:00:00|5.115.104.45, 5.1...|10553971611616272...|+53187-08-26 00:1...|          null|+53190-05-22 00:1...|            null|+53187-08-26 03:4...|          null|     12556.000000000|\n",
      "|60565df484a54c000...|34b6b677-ee7d-474...|     09120649010| 3000000.0000000000|1616272884324.000...|1616359284324.000...|1616272887272.000...|     8|          0E-10|CARD_TRANSFER|   MOBILE|          6374|       502229|             057|               2548|            502229|                  057|                 null| 27300654810|2021-03-21 00:00:00|188.229.10.195, 1...|17214583991616272...|+53187-08-26 06:3...|          null|+53190-05-22 06:3...|            null|+53187-08-26 07:2...|          null|      2948.000000000|\n",
      "|60565e0384a54c000...|9e6cce2a-0e4e-4dd...|     09211781720|  200000.0000000000|1616272899352.000...|1616359299352.000...|1616272902495.000...|     8|6000.0000000000|CARD_TRANSFER|   MOBILE|          3649|       504706|             061|               9364|            505416|                  064|                 null|        null|2021-03-21 00:00:00|185.8.172.200, 18...|10651491211616272...|+53187-08-26 10:4...|          null|+53190-05-22 10:4...|            null|+53187-08-26 11:3...|          null|      3143.000000000|\n",
      "|60565dfc629149000...|6bfeafdf-3ffc-45c...|     09126101191|10000000.0000000000|1616272892579.000...|1616359292579.000...|1616272902721.000...|     8|6000.0000000000|CARD_TRANSFER|   MOBILE|          8121|       589210|             015|               8294|            636949|                  065|                 null|     8202971|2021-03-21 00:00:00|5.114.58.164, 5.1...|14582985371616272...|+53187-08-26 08:5...|          null|+53190-05-22 08:5...|            null|+53187-08-26 11:4...|          null|     10142.000000000|\n",
      "|60577a0c7b3387000...|32c68834-c81c-47a...|     09109122258|13380000.0000000000|1616345612364.000...|1616432012364.000...|1616345615931.000...|     8|8400.0000000000|CARD_TRANSFER|   MOBILE|          3246|       507677|             080|               6219|            639599|                  052|                 null|222023337469|2021-03-21 00:00:00|5.215.153.202, 5....|87094191516163456...|+53189-12-14 23:4...|          null|+53192-09-10 00:4...|            null|+53189-12-15 00:4...|          null|      3567.000000000|\n",
      "|60577a1a84a54c000...|57771f75-43d6-4cc...|     09907299010|  500000.0000000000|1616345626255.000...|1616432026255.000...|1616345628308.000...|     8|          0E-10|CARD_TRANSFER|   MOBILE|          0227|       585983|             018|               0050|            585983|                  018|                 null|        null|2021-03-21 00:00:00|194.5.178.165, 19...|34082864116163456...|+53189-12-15 03:4...|          null|+53192-09-10 04:4...|            null|+53189-12-15 04:1...|          null|      2053.000000000|\n",
      "|60577a2584a54c000...|c46b79e2-b61e-4a7...|     09335193889| 1000000.0000000000|1616345637510.000...|1616432037510.000...|1616345641128.000...|     8|6000.0000000000|CARD_TRANSFER|   MOBILE|          1679|       585983|             018|               5668|            502229|                  057|                 null|        null|2021-03-21 00:00:00|46.167.131.97, 46...|37678399161634563...|+53189-12-15 06:4...|          null|+53192-09-10 07:4...|            null|+53189-12-15 07:4...|          null|      3618.000000000|\n",
      "|60577a29629149000...|d11242c7-9155-4a1...|     09126488509|10000000.0000000000|1616345641985.000...|1616432041985.000...|1616345643842.000...|     8|6000.0000000000|CARD_TRANSFER|   MOBILE|          0957|       610433|             012|               4997|            504706|                  061|                 null|107844018508|2021-03-21 00:00:00|89.45.61.82, 89.4...|10791493301616345...|+53189-12-15 08:0...|          null|+53192-09-10 09:0...|            null|+53189-12-15 08:3...|          null|      1857.000000000|\n",
      "|60577a2d629149000...|8cabee35-ab16-4e0...|     09381201370|  100000.0000000000|1616345645006.000...|1616432045006.000...|1616345647992.000...|     8|          0E-10|CARD_TRANSFER|   MOBILE|          4645|       627381|             063|               4421|            627381|                  063|                 null|130006292611|2021-03-21 00:00:00|5.126.121.38, 5.1...|94921200816163456...|+53189-12-15 08:5...|          null|+53192-09-10 09:5...|            null|+53189-12-15 09:4...|          null|      2986.000000000|\n",
      "+--------------------+--------------------+----------------+-------------------+--------------------+--------------------+--------------------+------+---------------+-------------+---------+--------------+-------------+----------------+-------------------+------------------+---------------------+---------------------+------------+-------------------+--------------------+--------------------+--------------------+--------------+--------------------+----------------+--------------------+--------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.sql(\"select * from parquet.c2c\")\n",
    "df.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6059\n",
      "+--------------------+--------------------+----------------+-------------------+--------------------+--------------------+--------------------+------+---------------+-------------+---------+--------------+-------------+----------------+-------------------+------------------+---------------------+---------------------+------------+-------------------+--------------------+--------------------+--------------------+--------------+--------------------+----------------+--------------------+--------------+--------------------+\n",
      "|                 AID|        OWNER_USERID|OWNER_CELLNUMBER|             AMOUNT|        CREATIONDATE|      EXPIRATIONDATE|        EXERCISEDATE|STATUS|      FEECHARGE|         TYPE|INITIATOR|SOURCE_POSTFIX|SOURCE_PREFIX|SOURCE_BANK_CODE|DESTINATION_POSTFIX|DESTINATION_PREFIX|DESTINATION_BANK_CODE|DESTINATION_CARDINDEX|         RRN|              GDATE|                  IP|        TRACKINGCODE|      creationdate_d|creationdate_j|    expirationdate_d|expirationdate_j|      exercisedate_d|exercisedate_j|diff_exercise_create|\n",
      "+--------------------+--------------------+----------------+-------------------+--------------------+--------------------+--------------------+------+---------------+-------------+---------+--------------+-------------+----------------+-------------------+------------------+---------------------+---------------------+------------+-------------------+--------------------+--------------------+--------------------+--------------+--------------------+----------------+--------------------+--------------+--------------------+\n",
      "|60565c55629149000...|7395dd6c-cae3-441...|     09126494166|13000000.0000000000|1616272469385.000...|1616358869385.000...|1616272474997.000...|     8|8400.0000000000|CARD_TRANSFER|   MOBILE|          6195|       504706|             061|               9355|            603770|                  016|                 null|        null|2021-03-21 00:00:00|2.147.59.5, 2.147...|16968857901616272...|+53187-08-21 11:1...|          null|+53190-05-17 11:1...|            null|+53187-08-21 12:5...|          null|      5612.000000000|\n",
      "|60565cc084a54c000...|6bacd64a-d9ab-499...|     09308118835| 2700000.0000000000|1616272576818.000...|1616358976818.000...|1616272578914.000...|     8|          0E-10|CARD_TRANSFER|   MOBILE|          1496|       622106|             054|               9036|            622106|                  054|                 null|       61351|2021-03-21 00:00:00|151.234.14.137, 1...|77383579016162725...|+53187-08-22 17:1...|          null|+53190-05-18 17:1...|            null|+53187-08-22 17:4...|          null|      2096.000000000|\n",
      "|60565cc57b3387000...|fe591dcd-2bac-424...|     09125607421| 2500000.0000000000|1616272581342.000...|1616358981342.000...|1616272584558.000...|     8|6000.0000000000|CARD_TRANSFER|   MOBILE|          5929|       585983|             018|               7305|            610433|                  012|                 null|        null|2021-03-21 00:00:00|89.38.102.5, 89.3...|59324525616162725...|+53187-08-22 18:2...|          null|+53190-05-18 18:2...|            null|+53187-08-22 19:1...|          null|      3216.000000000|\n",
      "|605772f8629149000...|b0837814-256d-4b5...|     09197298580| 2000000.0000000000|1616343800700.000...|1616430200700.000...|1616343804162.000...|     8|6000.0000000000|CARD_TRANSFER|   MOBILE|          1156|       502229|             057|               0622|            610433|                  012|                 null| 27309890255|2021-03-21 00:00:00|86.55.161.87, 86....|16790535681616343...|+53189-11-24 00:3...|          null|+53192-08-20 01:3...|            null|+53189-11-24 01:3...|          null|      3462.000000000|\n",
      "|605772f9629149000...|9a4f5456-84d6-484...|     09172703585|  460200.0000000000|1616343801497.000...|1616430201497.000...|1616343804527.000...|     8|6000.0000000000|CARD_TRANSFER|   MOBILE|          8528|       585983|             018|               1778|            627381|                  063|                 null|        null|2021-03-21 00:00:00|5.134.149.54, 5.1...|19614581611616343...|+53189-11-24 00:4...|          null|+53192-08-20 01:4...|            null|+53189-11-24 01:3...|          null|      3030.000000000|\n",
      "|605773007b3387000...|266c620a-5ded-411...|     09028589356| 5000000.0000000000|1616343808764.000...|1616430208764.000...|1616343810601.000...|     8|6000.0000000000|CARD_TRANSFER|   MOBILE|          9266|       610433|             012|               4203|            628023|                  014|                 null|107843220774|2021-03-21 00:00:00|95.162.155.10, 95...|11586064591616343...|+53189-11-24 02:4...|          null|+53192-08-20 03:4...|            null|+53189-11-24 03:2...|          null|      1837.000000000|\n",
      "|60577305629149000...|4b09ff23-805b-42e...|     09373737339| 3800000.0000000000|1616343813031.000...|1616430213031.000...|1616343815543.000...|     8|6000.0000000000|CARD_TRANSFER|   MOBILE|          6637|       622106|             054|               2347|            603799|                  017|                 null|      445351|2021-03-21 00:00:00|89.45.126.144, 89...|14870134591616343...|+53189-11-24 04:0...|          null|+53192-08-20 05:0...|            null|+53189-11-24 04:4...|          null|      2512.000000000|\n",
      "|60565d047b3387000...|ad7ab34e-e00c-42e...|     09195560293|10000000.0000000000|1616272644724.000...|1616359044724.000...|1616272651339.000...|     8|6000.0000000000|CARD_TRANSFER|   MOBILE|          4952|       504706|             061|               7459|            628023|                  014|                 null|        null|2021-03-21 00:00:00|89.196.73.168, 89...|65216752016162726...|+53187-08-23 12:0...|          null|+53190-05-19 12:0...|            null|+53187-08-23 13:5...|          null|      6615.000000000|\n",
      "|60565d10629149000...|4ba2af4a-a658-41c...|     09187109687| 2600000.0000000000|1616272656581.000...|1616359056581.000...|1616272662507.000...|     8|6000.0000000000|CARD_TRANSFER|   MOBILE|          6889|       636949|             065|               2629|            610433|                  012|                 null|108000745003|2021-03-21 00:00:00|5.124.54.251, 5.1...|19861407191616272...|+53187-08-23 15:1...|          null|+53190-05-19 15:1...|            null|+53187-08-23 16:5...|          null|      5926.000000000|\n",
      "|60565d2f629149000...|6bacd64a-d9ab-499...|     09308118835| 3800000.0000000000|1616272687310.000...|1616359087310.000...|1616272689969.000...|     8|6000.0000000000|CARD_TRANSFER|   MOBILE|          1496|       622106|             054|               0024|            628023|                  014|                 null|       61367|2021-03-21 00:00:00|151.234.14.137, 1...|28768904916162726...|+53187-08-23 23:5...|          null|+53190-05-19 23:5...|            null|+53187-08-24 00:3...|          null|      2659.000000000|\n",
      "|60565d65629149000...|ea52ac93-76ea-48d...|     09910933197|10000000.0000000000|1616272741350.000...|1616359141350.000...|1616272745852.000...|     8|6000.0000000000|CARD_TRANSFER|   MOBILE|          9999|       627381|             063|               8868|            589210|                  015|                 null|130004998136|2021-03-21 00:00:00|83.120.67.197, 83...|11047406131616272...|+53187-08-24 14:5...|          null|+53190-05-20 14:5...|            null|+53187-08-24 16:0...|          null|      4502.000000000|\n",
      "|60565ddd629149000...|5ae65f46-ef52-4ae...|     09035093668|  900000.0000000000|1616272861613.000...|1616359261613.000...|1616272874169.000...|     8|6000.0000000000|CARD_TRANSFER|   MOBILE|          4819|       504706|             061|               9311|            589210|                  015|                 null|        null|2021-03-21 00:00:00|5.115.104.45, 5.1...|10553971611616272...|+53187-08-26 00:1...|          null|+53190-05-22 00:1...|            null|+53187-08-26 03:4...|          null|     12556.000000000|\n",
      "|60565df484a54c000...|34b6b677-ee7d-474...|     09120649010| 3000000.0000000000|1616272884324.000...|1616359284324.000...|1616272887272.000...|     8|          0E-10|CARD_TRANSFER|   MOBILE|          6374|       502229|             057|               2548|            502229|                  057|                 null| 27300654810|2021-03-21 00:00:00|188.229.10.195, 1...|17214583991616272...|+53187-08-26 06:3...|          null|+53190-05-22 06:3...|            null|+53187-08-26 07:2...|          null|      2948.000000000|\n",
      "|60565e0384a54c000...|9e6cce2a-0e4e-4dd...|     09211781720|  200000.0000000000|1616272899352.000...|1616359299352.000...|1616272902495.000...|     8|6000.0000000000|CARD_TRANSFER|   MOBILE|          3649|       504706|             061|               9364|            505416|                  064|                 null|        null|2021-03-21 00:00:00|185.8.172.200, 18...|10651491211616272...|+53187-08-26 10:4...|          null|+53190-05-22 10:4...|            null|+53187-08-26 11:3...|          null|      3143.000000000|\n",
      "|60565dfc629149000...|6bfeafdf-3ffc-45c...|     09126101191|10000000.0000000000|1616272892579.000...|1616359292579.000...|1616272902721.000...|     8|6000.0000000000|CARD_TRANSFER|   MOBILE|          8121|       589210|             015|               8294|            636949|                  065|                 null|     8202971|2021-03-21 00:00:00|5.114.58.164, 5.1...|14582985371616272...|+53187-08-26 08:5...|          null|+53190-05-22 08:5...|            null|+53187-08-26 11:4...|          null|     10142.000000000|\n",
      "|60577a0c7b3387000...|32c68834-c81c-47a...|     09109122258|13380000.0000000000|1616345612364.000...|1616432012364.000...|1616345615931.000...|     8|8400.0000000000|CARD_TRANSFER|   MOBILE|          3246|       507677|             080|               6219|            639599|                  052|                 null|222023337469|2021-03-21 00:00:00|5.215.153.202, 5....|87094191516163456...|+53189-12-14 23:4...|          null|+53192-09-10 00:4...|            null|+53189-12-15 00:4...|          null|      3567.000000000|\n",
      "|60577a1a84a54c000...|57771f75-43d6-4cc...|     09907299010|  500000.0000000000|1616345626255.000...|1616432026255.000...|1616345628308.000...|     8|          0E-10|CARD_TRANSFER|   MOBILE|          0227|       585983|             018|               0050|            585983|                  018|                 null|        null|2021-03-21 00:00:00|194.5.178.165, 19...|34082864116163456...|+53189-12-15 03:4...|          null|+53192-09-10 04:4...|            null|+53189-12-15 04:1...|          null|      2053.000000000|\n",
      "|60577a2584a54c000...|c46b79e2-b61e-4a7...|     09335193889| 1000000.0000000000|1616345637510.000...|1616432037510.000...|1616345641128.000...|     8|6000.0000000000|CARD_TRANSFER|   MOBILE|          1679|       585983|             018|               5668|            502229|                  057|                 null|        null|2021-03-21 00:00:00|46.167.131.97, 46...|37678399161634563...|+53189-12-15 06:4...|          null|+53192-09-10 07:4...|            null|+53189-12-15 07:4...|          null|      3618.000000000|\n",
      "|60577a29629149000...|d11242c7-9155-4a1...|     09126488509|10000000.0000000000|1616345641985.000...|1616432041985.000...|1616345643842.000...|     8|6000.0000000000|CARD_TRANSFER|   MOBILE|          0957|       610433|             012|               4997|            504706|                  061|                 null|107844018508|2021-03-21 00:00:00|89.45.61.82, 89.4...|10791493301616345...|+53189-12-15 08:0...|          null|+53192-09-10 09:0...|            null|+53189-12-15 08:3...|          null|      1857.000000000|\n",
      "|60577a2d629149000...|8cabee35-ab16-4e0...|     09381201370|  100000.0000000000|1616345645006.000...|1616432045006.000...|1616345647992.000...|     8|          0E-10|CARD_TRANSFER|   MOBILE|          4645|       627381|             063|               4421|            627381|                  063|                 null|130006292611|2021-03-21 00:00:00|5.126.121.38, 5.1...|94921200816163456...|+53189-12-15 08:5...|          null|+53192-09-10 09:5...|            null|+53189-12-15 09:4...|          null|      2986.000000000|\n",
      "+--------------------+--------------------+----------------+-------------------+--------------------+--------------------+--------------------+------+---------------+-------------+---------+--------------+-------------+----------------+-------------------+------------------+---------------------+---------------------+------------+-------------------+--------------------+--------------------+--------------------+--------------+--------------------+----------------+--------------------+--------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(df.count())\n",
    "df.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------+------------------------------------+----------------+-------------------+------------------------+------------------------+------------------------+------+---------------+-------------+---------+--------------+-------------+----------------+-------------------+------------------+---------------------+---------------------+------------+-------------------+------------------------------+-----------------------+---------------------+--------------+---------------------+----------------+---------------------+--------------+--------------------+-----+\n",
      "|AID                     |OWNER_USERID                        |OWNER_CELLNUMBER|AMOUNT             |CREATIONDATE            |EXPIRATIONDATE          |EXERCISEDATE            |STATUS|FEECHARGE      |TYPE         |INITIATOR|SOURCE_POSTFIX|SOURCE_PREFIX|SOURCE_BANK_CODE|DESTINATION_POSTFIX|DESTINATION_PREFIX|DESTINATION_BANK_CODE|DESTINATION_CARDINDEX|RRN         |GDATE              |IP                            |TRACKINGCODE           |creationdate_d       |creationdate_j|expirationdate_d     |expirationdate_j|exercisedate_d       |exercisedate_j|diff_exercise_create|count|\n",
      "+------------------------+------------------------------------+----------------+-------------------+------------------------+------------------------+------------------------+------+---------------+-------------+---------+--------------+-------------+----------------+-------------------+------------------+---------------------+---------------------+------------+-------------------+------------------------------+-----------------------+---------------------+--------------+---------------------+----------------+---------------------+--------------+--------------------+-----+\n",
      "|60565c556291490001dbff2e|7395dd6c-cae3-4410-84f0-3e337ac046d1|09126494166     |13000000.0000000000|1616272469385.0000000000|1616358869385.0000000000|1616272474997.0000000000|8     |8400.0000000000|CARD_TRANSFER|MOBILE   |6195          |504706       |061             |9355               |603770            |016                  |null                 |null        |2021-03-21 00:00:00|2.147.59.5, 2.147.59.5        |16968857901616272469385|+53187-08-21 11:19:45|null          |+53190-05-17 11:19:45|null            |+53187-08-21 12:53:17|null          |5612.000000000      |2    |\n",
      "|60565cc084a54c0001825c7f|6bacd64a-d9ab-4999-abf6-fe69c2d8dbe0|09308118835     |2700000.0000000000 |1616272576818.0000000000|1616358976818.0000000000|1616272578914.0000000000|8     |0E-10          |CARD_TRANSFER|MOBILE   |1496          |622106       |054             |9036               |622106            |054                  |null                 |61351       |2021-03-21 00:00:00|151.234.14.137, 151.234.14.137|7738357901616272576819 |+53187-08-22 17:10:18|null          |+53190-05-18 17:10:18|null            |+53187-08-22 17:45:14|null          |2096.000000000      |2    |\n",
      "|60565cc57b338700018d5245|fe591dcd-2bac-4241-8127-c5b7f44df523|09125607421     |2500000.0000000000 |1616272581342.0000000000|1616358981342.0000000000|1616272584558.0000000000|8     |6000.0000000000|CARD_TRANSFER|MOBILE   |5929          |585983       |018             |7305               |610433            |012                  |null                 |null        |2021-03-21 00:00:00|89.38.102.5, 89.38.102.5      |5932452561616272581342 |+53187-08-22 18:25:42|null          |+53190-05-18 18:25:42|null            |+53187-08-22 19:19:18|null          |3216.000000000      |2    |\n",
      "|605772f86291490001dc1774|b0837814-256d-4b55-a407-541ce8c0c4c5|09197298580     |2000000.0000000000 |1616343800700.0000000000|1616430200700.0000000000|1616343804162.0000000000|8     |6000.0000000000|CARD_TRANSFER|MOBILE   |1156          |502229       |057             |0622               |610433            |012                  |null                 |27309890255 |2021-03-21 00:00:00|86.55.161.87, 86.55.161.87    |16790535681616343800700|+53189-11-24 00:35:00|null          |+53192-08-20 01:35:00|null            |+53189-11-24 01:32:42|null          |3462.000000000      |2    |\n",
      "|605772f96291490001dc1775|9a4f5456-84d6-484d-8859-a2c218cb7065|09172703585     |460200.0000000000  |1616343801497.0000000000|1616430201497.0000000000|1616343804527.0000000000|8     |6000.0000000000|CARD_TRANSFER|MOBILE   |8528          |585983       |018             |1778               |627381            |063                  |null                 |null        |2021-03-21 00:00:00|5.134.149.54, 5.134.149.54    |19614581611616343801498|+53189-11-24 00:48:17|null          |+53192-08-20 01:48:17|null            |+53189-11-24 01:38:47|null          |3030.000000000      |2    |\n",
      "|605773007b338700018d6ab5|266c620a-5ded-4110-a574-39a11fb73e6b|09028589356     |5000000.0000000000 |1616343808764.0000000000|1616430208764.0000000000|1616343810601.0000000000|8     |6000.0000000000|CARD_TRANSFER|MOBILE   |9266          |610433       |012             |4203               |628023            |014                  |null                 |107843220774|2021-03-21 00:00:00|95.162.155.10, 95.162.155.10  |11586064591616343808764|+53189-11-24 02:49:24|null          |+53192-08-20 03:49:24|null            |+53189-11-24 03:20:01|null          |1837.000000000      |2    |\n",
      "|605773056291490001dc1777|4b09ff23-805b-42e3-86a3-5c68d69703e2|09373737339     |3800000.0000000000 |1616343813031.0000000000|1616430213031.0000000000|1616343815543.0000000000|8     |6000.0000000000|CARD_TRANSFER|MOBILE   |6637          |622106       |054             |2347               |603799            |017                  |null                 |445351      |2021-03-21 00:00:00|89.45.126.144, 89.45.126.144  |14870134591616343813032|+53189-11-24 04:00:31|null          |+53192-08-20 05:00:31|null            |+53189-11-24 04:42:23|null          |2512.000000000      |2    |\n",
      "|60565d047b338700018d524b|ad7ab34e-e00c-42ea-a884-cea8b0982b08|09195560293     |10000000.0000000000|1616272644724.0000000000|1616359044724.0000000000|1616272651339.0000000000|8     |6000.0000000000|CARD_TRANSFER|MOBILE   |4952          |504706       |061             |7459               |628023            |014                  |null                 |null        |2021-03-21 00:00:00|89.196.73.168, 89.196.73.168  |6521675201616272644724 |+53187-08-23 12:02:04|null          |+53190-05-19 12:02:04|null            |+53187-08-23 13:52:19|null          |6615.000000000      |2    |\n",
      "|60565d106291490001dbff48|4ba2af4a-a658-41cc-ba59-61d379b059df|09187109687     |2600000.0000000000 |1616272656581.0000000000|1616359056581.0000000000|1616272662507.0000000000|8     |6000.0000000000|CARD_TRANSFER|MOBILE   |6889          |636949       |065             |2629               |610433            |012                  |null                 |108000745003|2021-03-21 00:00:00|5.124.54.251, 5.124.54.251    |19861407191616272656582|+53187-08-23 15:19:41|null          |+53190-05-19 15:19:41|null            |+53187-08-23 16:58:27|null          |5926.000000000      |2    |\n",
      "|60565d2f6291490001dbff4f|6bacd64a-d9ab-4999-abf6-fe69c2d8dbe0|09308118835     |3800000.0000000000 |1616272687310.0000000000|1616359087310.0000000000|1616272689969.0000000000|8     |6000.0000000000|CARD_TRANSFER|MOBILE   |1496          |622106       |054             |0024               |628023            |014                  |null                 |61367       |2021-03-21 00:00:00|151.234.14.137, 151.234.14.137|2876890491616272687310 |+53187-08-23 23:51:50|null          |+53190-05-19 23:51:50|null            |+53187-08-24 00:36:09|null          |2659.000000000      |2    |\n",
      "+------------------------+------------------------------------+----------------+-------------------+------------------------+------------------------+------------------------+------+---------------+-------------+---------+--------------+-------------+----------------+-------------------+------------------+---------------------+---------------------+------------+-------------------+------------------------------+-----------------------+---------------------+--------------+---------------------+----------------+---------------------+--------------+--------------------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2 = df.withColumn('count', functions.size(functions.split('ip', ',')))\n",
    "df2.show(10, False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+\n",
      "|min(count)|max(count)|\n",
      "+----------+----------+\n",
      "|2         |2         |\n",
      "+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.groupBy().agg(functions.min('count'), functions.max('count')).show(10, False)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------+------------------------------------+----------------+-------------------+------------------------+------------------------+------------------------+------+---------------+-------------+---------+--------------+-------------+----------------+-------------------+------------------+---------------------+---------------------+------------+-------------------+------------------------------+-----------------------+---------------------+--------------+---------------------+----------------+---------------------+--------------+--------------------+--------------+---------------+----+\n",
      "|AID                     |OWNER_USERID                        |OWNER_CELLNUMBER|AMOUNT             |CREATIONDATE            |EXPIRATIONDATE          |EXERCISEDATE            |STATUS|FEECHARGE      |TYPE         |INITIATOR|SOURCE_POSTFIX|SOURCE_PREFIX|SOURCE_BANK_CODE|DESTINATION_POSTFIX|DESTINATION_PREFIX|DESTINATION_BANK_CODE|DESTINATION_CARDINDEX|RRN         |GDATE              |IP                            |TRACKINGCODE           |creationdate_d       |creationdate_j|expirationdate_d     |expirationdate_j|exercisedate_d       |exercisedate_j|diff_exercise_create|ip1           |ip2            |ip3 |\n",
      "+------------------------+------------------------------------+----------------+-------------------+------------------------+------------------------+------------------------+------+---------------+-------------+---------+--------------+-------------+----------------+-------------------+------------------+---------------------+---------------------+------------+-------------------+------------------------------+-----------------------+---------------------+--------------+---------------------+----------------+---------------------+--------------+--------------------+--------------+---------------+----+\n",
      "|60565c556291490001dbff2e|7395dd6c-cae3-4410-84f0-3e337ac046d1|09126494166     |13000000.0000000000|1616272469385.0000000000|1616358869385.0000000000|1616272474997.0000000000|8     |8400.0000000000|CARD_TRANSFER|MOBILE   |6195          |504706       |061             |9355               |603770            |016                  |null                 |null        |2021-03-21 00:00:00|2.147.59.5, 2.147.59.5        |16968857901616272469385|+53187-08-21 11:19:45|null          |+53190-05-17 11:19:45|null            |+53187-08-21 12:53:17|null          |5612.000000000      |2.147.59.5    | 2.147.59.5    |null|\n",
      "|60565cc084a54c0001825c7f|6bacd64a-d9ab-4999-abf6-fe69c2d8dbe0|09308118835     |2700000.0000000000 |1616272576818.0000000000|1616358976818.0000000000|1616272578914.0000000000|8     |0E-10          |CARD_TRANSFER|MOBILE   |1496          |622106       |054             |9036               |622106            |054                  |null                 |61351       |2021-03-21 00:00:00|151.234.14.137, 151.234.14.137|7738357901616272576819 |+53187-08-22 17:10:18|null          |+53190-05-18 17:10:18|null            |+53187-08-22 17:45:14|null          |2096.000000000      |151.234.14.137| 151.234.14.137|null|\n",
      "|60565cc57b338700018d5245|fe591dcd-2bac-4241-8127-c5b7f44df523|09125607421     |2500000.0000000000 |1616272581342.0000000000|1616358981342.0000000000|1616272584558.0000000000|8     |6000.0000000000|CARD_TRANSFER|MOBILE   |5929          |585983       |018             |7305               |610433            |012                  |null                 |null        |2021-03-21 00:00:00|89.38.102.5, 89.38.102.5      |5932452561616272581342 |+53187-08-22 18:25:42|null          |+53190-05-18 18:25:42|null            |+53187-08-22 19:19:18|null          |3216.000000000      |89.38.102.5   | 89.38.102.5   |null|\n",
      "|605772f86291490001dc1774|b0837814-256d-4b55-a407-541ce8c0c4c5|09197298580     |2000000.0000000000 |1616343800700.0000000000|1616430200700.0000000000|1616343804162.0000000000|8     |6000.0000000000|CARD_TRANSFER|MOBILE   |1156          |502229       |057             |0622               |610433            |012                  |null                 |27309890255 |2021-03-21 00:00:00|86.55.161.87, 86.55.161.87    |16790535681616343800700|+53189-11-24 00:35:00|null          |+53192-08-20 01:35:00|null            |+53189-11-24 01:32:42|null          |3462.000000000      |86.55.161.87  | 86.55.161.87  |null|\n",
      "|605772f96291490001dc1775|9a4f5456-84d6-484d-8859-a2c218cb7065|09172703585     |460200.0000000000  |1616343801497.0000000000|1616430201497.0000000000|1616343804527.0000000000|8     |6000.0000000000|CARD_TRANSFER|MOBILE   |8528          |585983       |018             |1778               |627381            |063                  |null                 |null        |2021-03-21 00:00:00|5.134.149.54, 5.134.149.54    |19614581611616343801498|+53189-11-24 00:48:17|null          |+53192-08-20 01:48:17|null            |+53189-11-24 01:38:47|null          |3030.000000000      |5.134.149.54  | 5.134.149.54  |null|\n",
      "|605773007b338700018d6ab5|266c620a-5ded-4110-a574-39a11fb73e6b|09028589356     |5000000.0000000000 |1616343808764.0000000000|1616430208764.0000000000|1616343810601.0000000000|8     |6000.0000000000|CARD_TRANSFER|MOBILE   |9266          |610433       |012             |4203               |628023            |014                  |null                 |107843220774|2021-03-21 00:00:00|95.162.155.10, 95.162.155.10  |11586064591616343808764|+53189-11-24 02:49:24|null          |+53192-08-20 03:49:24|null            |+53189-11-24 03:20:01|null          |1837.000000000      |95.162.155.10 | 95.162.155.10 |null|\n",
      "|605773056291490001dc1777|4b09ff23-805b-42e3-86a3-5c68d69703e2|09373737339     |3800000.0000000000 |1616343813031.0000000000|1616430213031.0000000000|1616343815543.0000000000|8     |6000.0000000000|CARD_TRANSFER|MOBILE   |6637          |622106       |054             |2347               |603799            |017                  |null                 |445351      |2021-03-21 00:00:00|89.45.126.144, 89.45.126.144  |14870134591616343813032|+53189-11-24 04:00:31|null          |+53192-08-20 05:00:31|null            |+53189-11-24 04:42:23|null          |2512.000000000      |89.45.126.144 | 89.45.126.144 |null|\n",
      "|60565d047b338700018d524b|ad7ab34e-e00c-42ea-a884-cea8b0982b08|09195560293     |10000000.0000000000|1616272644724.0000000000|1616359044724.0000000000|1616272651339.0000000000|8     |6000.0000000000|CARD_TRANSFER|MOBILE   |4952          |504706       |061             |7459               |628023            |014                  |null                 |null        |2021-03-21 00:00:00|89.196.73.168, 89.196.73.168  |6521675201616272644724 |+53187-08-23 12:02:04|null          |+53190-05-19 12:02:04|null            |+53187-08-23 13:52:19|null          |6615.000000000      |89.196.73.168 | 89.196.73.168 |null|\n",
      "|60565d106291490001dbff48|4ba2af4a-a658-41cc-ba59-61d379b059df|09187109687     |2600000.0000000000 |1616272656581.0000000000|1616359056581.0000000000|1616272662507.0000000000|8     |6000.0000000000|CARD_TRANSFER|MOBILE   |6889          |636949       |065             |2629               |610433            |012                  |null                 |108000745003|2021-03-21 00:00:00|5.124.54.251, 5.124.54.251    |19861407191616272656582|+53187-08-23 15:19:41|null          |+53190-05-19 15:19:41|null            |+53187-08-23 16:58:27|null          |5926.000000000      |5.124.54.251  | 5.124.54.251  |null|\n",
      "|60565d2f6291490001dbff4f|6bacd64a-d9ab-4999-abf6-fe69c2d8dbe0|09308118835     |3800000.0000000000 |1616272687310.0000000000|1616359087310.0000000000|1616272689969.0000000000|8     |6000.0000000000|CARD_TRANSFER|MOBILE   |1496          |622106       |054             |0024               |628023            |014                  |null                 |61367       |2021-03-21 00:00:00|151.234.14.137, 151.234.14.137|2876890491616272687310 |+53187-08-23 23:51:50|null          |+53190-05-19 23:51:50|null            |+53187-08-24 00:36:09|null          |2659.000000000      |151.234.14.137| 151.234.14.137|null|\n",
      "+------------------------+------------------------------------+----------------+-------------------+------------------------+------------------------+------------------------+------+---------------+-------------+---------+--------------+-------------+----------------+-------------------+------------------+---------------------+---------------------+------------+-------------------+------------------------------+-----------------------+---------------------+--------------+---------------------+----------------+---------------------+--------------+--------------------+--------------+---------------+----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2 = df.withColumn('ip1', functions.split('ip', ',')[0]) \\\n",
    "    .withColumn('ip2', functions.split('ip', ',')[1]) \\\n",
    "    .withColumn('ip3', functions.split('ip', ',')[2])\n",
    "df2.show(10, False)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "data": {
      "text/plain": "DataFrame[]"
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --Use data from another table\n",
    "# *****************************\n",
    "spark.sql(\"DROP TABLE IF EXISTS c2c_copy\")\n",
    "spark.sql(\"CREATE TABLE c2c_copy using parquet \"\n",
    "          \" AS SELECT * FROM parquet.c2c \")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6059\n",
      "+--------------------+--------------------+----------------+-------------------+--------------------+--------------------+--------------------+------+---------------+-------------+---------+--------------+-------------+----------------+-------------------+------------------+---------------------+---------------------+------------+-------------------+--------------------+--------------------+--------------------+--------------+--------------------+----------------+--------------------+--------------+--------------------+\n",
      "|                 AID|        OWNER_USERID|OWNER_CELLNUMBER|             AMOUNT|        CREATIONDATE|      EXPIRATIONDATE|        EXERCISEDATE|STATUS|      FEECHARGE|         TYPE|INITIATOR|SOURCE_POSTFIX|SOURCE_PREFIX|SOURCE_BANK_CODE|DESTINATION_POSTFIX|DESTINATION_PREFIX|DESTINATION_BANK_CODE|DESTINATION_CARDINDEX|         RRN|              GDATE|                  IP|        TRACKINGCODE|      creationdate_d|creationdate_j|    expirationdate_d|expirationdate_j|      exercisedate_d|exercisedate_j|diff_exercise_create|\n",
      "+--------------------+--------------------+----------------+-------------------+--------------------+--------------------+--------------------+------+---------------+-------------+---------+--------------+-------------+----------------+-------------------+------------------+---------------------+---------------------+------------+-------------------+--------------------+--------------------+--------------------+--------------+--------------------+----------------+--------------------+--------------+--------------------+\n",
      "|60565c55629149000...|7395dd6c-cae3-441...|     09126494166|13000000.0000000000|1616272469385.000...|1616358869385.000...|1616272474997.000...|     8|8400.0000000000|CARD_TRANSFER|   MOBILE|          6195|       504706|             061|               9355|            603770|                  016|                 null|        null|2021-03-21 00:00:00|2.147.59.5, 2.147...|16968857901616272...|+53187-08-21 11:1...|          null|+53190-05-17 11:1...|            null|+53187-08-21 12:5...|          null|      5612.000000000|\n",
      "|60565cc084a54c000...|6bacd64a-d9ab-499...|     09308118835| 2700000.0000000000|1616272576818.000...|1616358976818.000...|1616272578914.000...|     8|          0E-10|CARD_TRANSFER|   MOBILE|          1496|       622106|             054|               9036|            622106|                  054|                 null|       61351|2021-03-21 00:00:00|151.234.14.137, 1...|77383579016162725...|+53187-08-22 17:1...|          null|+53190-05-18 17:1...|            null|+53187-08-22 17:4...|          null|      2096.000000000|\n",
      "|60565cc57b3387000...|fe591dcd-2bac-424...|     09125607421| 2500000.0000000000|1616272581342.000...|1616358981342.000...|1616272584558.000...|     8|6000.0000000000|CARD_TRANSFER|   MOBILE|          5929|       585983|             018|               7305|            610433|                  012|                 null|        null|2021-03-21 00:00:00|89.38.102.5, 89.3...|59324525616162725...|+53187-08-22 18:2...|          null|+53190-05-18 18:2...|            null|+53187-08-22 19:1...|          null|      3216.000000000|\n",
      "|605772f8629149000...|b0837814-256d-4b5...|     09197298580| 2000000.0000000000|1616343800700.000...|1616430200700.000...|1616343804162.000...|     8|6000.0000000000|CARD_TRANSFER|   MOBILE|          1156|       502229|             057|               0622|            610433|                  012|                 null| 27309890255|2021-03-21 00:00:00|86.55.161.87, 86....|16790535681616343...|+53189-11-24 00:3...|          null|+53192-08-20 01:3...|            null|+53189-11-24 01:3...|          null|      3462.000000000|\n",
      "|605772f9629149000...|9a4f5456-84d6-484...|     09172703585|  460200.0000000000|1616343801497.000...|1616430201497.000...|1616343804527.000...|     8|6000.0000000000|CARD_TRANSFER|   MOBILE|          8528|       585983|             018|               1778|            627381|                  063|                 null|        null|2021-03-21 00:00:00|5.134.149.54, 5.1...|19614581611616343...|+53189-11-24 00:4...|          null|+53192-08-20 01:4...|            null|+53189-11-24 01:3...|          null|      3030.000000000|\n",
      "|605773007b3387000...|266c620a-5ded-411...|     09028589356| 5000000.0000000000|1616343808764.000...|1616430208764.000...|1616343810601.000...|     8|6000.0000000000|CARD_TRANSFER|   MOBILE|          9266|       610433|             012|               4203|            628023|                  014|                 null|107843220774|2021-03-21 00:00:00|95.162.155.10, 95...|11586064591616343...|+53189-11-24 02:4...|          null|+53192-08-20 03:4...|            null|+53189-11-24 03:2...|          null|      1837.000000000|\n",
      "|60577305629149000...|4b09ff23-805b-42e...|     09373737339| 3800000.0000000000|1616343813031.000...|1616430213031.000...|1616343815543.000...|     8|6000.0000000000|CARD_TRANSFER|   MOBILE|          6637|       622106|             054|               2347|            603799|                  017|                 null|      445351|2021-03-21 00:00:00|89.45.126.144, 89...|14870134591616343...|+53189-11-24 04:0...|          null|+53192-08-20 05:0...|            null|+53189-11-24 04:4...|          null|      2512.000000000|\n",
      "|60565d047b3387000...|ad7ab34e-e00c-42e...|     09195560293|10000000.0000000000|1616272644724.000...|1616359044724.000...|1616272651339.000...|     8|6000.0000000000|CARD_TRANSFER|   MOBILE|          4952|       504706|             061|               7459|            628023|                  014|                 null|        null|2021-03-21 00:00:00|89.196.73.168, 89...|65216752016162726...|+53187-08-23 12:0...|          null|+53190-05-19 12:0...|            null|+53187-08-23 13:5...|          null|      6615.000000000|\n",
      "|60565d10629149000...|4ba2af4a-a658-41c...|     09187109687| 2600000.0000000000|1616272656581.000...|1616359056581.000...|1616272662507.000...|     8|6000.0000000000|CARD_TRANSFER|   MOBILE|          6889|       636949|             065|               2629|            610433|                  012|                 null|108000745003|2021-03-21 00:00:00|5.124.54.251, 5.1...|19861407191616272...|+53187-08-23 15:1...|          null|+53190-05-19 15:1...|            null|+53187-08-23 16:5...|          null|      5926.000000000|\n",
      "|60565d2f629149000...|6bacd64a-d9ab-499...|     09308118835| 3800000.0000000000|1616272687310.000...|1616359087310.000...|1616272689969.000...|     8|6000.0000000000|CARD_TRANSFER|   MOBILE|          1496|       622106|             054|               0024|            628023|                  014|                 null|       61367|2021-03-21 00:00:00|151.234.14.137, 1...|28768904916162726...|+53187-08-23 23:5...|          null|+53190-05-19 23:5...|            null|+53187-08-24 00:3...|          null|      2659.000000000|\n",
      "|60565d65629149000...|ea52ac93-76ea-48d...|     09910933197|10000000.0000000000|1616272741350.000...|1616359141350.000...|1616272745852.000...|     8|6000.0000000000|CARD_TRANSFER|   MOBILE|          9999|       627381|             063|               8868|            589210|                  015|                 null|130004998136|2021-03-21 00:00:00|83.120.67.197, 83...|11047406131616272...|+53187-08-24 14:5...|          null|+53190-05-20 14:5...|            null|+53187-08-24 16:0...|          null|      4502.000000000|\n",
      "|60565ddd629149000...|5ae65f46-ef52-4ae...|     09035093668|  900000.0000000000|1616272861613.000...|1616359261613.000...|1616272874169.000...|     8|6000.0000000000|CARD_TRANSFER|   MOBILE|          4819|       504706|             061|               9311|            589210|                  015|                 null|        null|2021-03-21 00:00:00|5.115.104.45, 5.1...|10553971611616272...|+53187-08-26 00:1...|          null|+53190-05-22 00:1...|            null|+53187-08-26 03:4...|          null|     12556.000000000|\n",
      "|60565df484a54c000...|34b6b677-ee7d-474...|     09120649010| 3000000.0000000000|1616272884324.000...|1616359284324.000...|1616272887272.000...|     8|          0E-10|CARD_TRANSFER|   MOBILE|          6374|       502229|             057|               2548|            502229|                  057|                 null| 27300654810|2021-03-21 00:00:00|188.229.10.195, 1...|17214583991616272...|+53187-08-26 06:3...|          null|+53190-05-22 06:3...|            null|+53187-08-26 07:2...|          null|      2948.000000000|\n",
      "|60565e0384a54c000...|9e6cce2a-0e4e-4dd...|     09211781720|  200000.0000000000|1616272899352.000...|1616359299352.000...|1616272902495.000...|     8|6000.0000000000|CARD_TRANSFER|   MOBILE|          3649|       504706|             061|               9364|            505416|                  064|                 null|        null|2021-03-21 00:00:00|185.8.172.200, 18...|10651491211616272...|+53187-08-26 10:4...|          null|+53190-05-22 10:4...|            null|+53187-08-26 11:3...|          null|      3143.000000000|\n",
      "|60565dfc629149000...|6bfeafdf-3ffc-45c...|     09126101191|10000000.0000000000|1616272892579.000...|1616359292579.000...|1616272902721.000...|     8|6000.0000000000|CARD_TRANSFER|   MOBILE|          8121|       589210|             015|               8294|            636949|                  065|                 null|     8202971|2021-03-21 00:00:00|5.114.58.164, 5.1...|14582985371616272...|+53187-08-26 08:5...|          null|+53190-05-22 08:5...|            null|+53187-08-26 11:4...|          null|     10142.000000000|\n",
      "|60577a0c7b3387000...|32c68834-c81c-47a...|     09109122258|13380000.0000000000|1616345612364.000...|1616432012364.000...|1616345615931.000...|     8|8400.0000000000|CARD_TRANSFER|   MOBILE|          3246|       507677|             080|               6219|            639599|                  052|                 null|222023337469|2021-03-21 00:00:00|5.215.153.202, 5....|87094191516163456...|+53189-12-14 23:4...|          null|+53192-09-10 00:4...|            null|+53189-12-15 00:4...|          null|      3567.000000000|\n",
      "|60577a1a84a54c000...|57771f75-43d6-4cc...|     09907299010|  500000.0000000000|1616345626255.000...|1616432026255.000...|1616345628308.000...|     8|          0E-10|CARD_TRANSFER|   MOBILE|          0227|       585983|             018|               0050|            585983|                  018|                 null|        null|2021-03-21 00:00:00|194.5.178.165, 19...|34082864116163456...|+53189-12-15 03:4...|          null|+53192-09-10 04:4...|            null|+53189-12-15 04:1...|          null|      2053.000000000|\n",
      "|60577a2584a54c000...|c46b79e2-b61e-4a7...|     09335193889| 1000000.0000000000|1616345637510.000...|1616432037510.000...|1616345641128.000...|     8|6000.0000000000|CARD_TRANSFER|   MOBILE|          1679|       585983|             018|               5668|            502229|                  057|                 null|        null|2021-03-21 00:00:00|46.167.131.97, 46...|37678399161634563...|+53189-12-15 06:4...|          null|+53192-09-10 07:4...|            null|+53189-12-15 07:4...|          null|      3618.000000000|\n",
      "|60577a29629149000...|d11242c7-9155-4a1...|     09126488509|10000000.0000000000|1616345641985.000...|1616432041985.000...|1616345643842.000...|     8|6000.0000000000|CARD_TRANSFER|   MOBILE|          0957|       610433|             012|               4997|            504706|                  061|                 null|107844018508|2021-03-21 00:00:00|89.45.61.82, 89.4...|10791493301616345...|+53189-12-15 08:0...|          null|+53192-09-10 09:0...|            null|+53189-12-15 08:3...|          null|      1857.000000000|\n",
      "|60577a2d629149000...|8cabee35-ab16-4e0...|     09381201370|  100000.0000000000|1616345645006.000...|1616432045006.000...|1616345647992.000...|     8|          0E-10|CARD_TRANSFER|   MOBILE|          4645|       627381|             063|               4421|            627381|                  063|                 null|130006292611|2021-03-21 00:00:00|5.126.121.38, 5.1...|94921200816163456...|+53189-12-15 08:5...|          null|+53192-09-10 09:5...|            null|+53189-12-15 09:4...|          null|      2986.000000000|\n",
      "+--------------------+--------------------+----------------+-------------------+--------------------+--------------------+--------------------+------+---------------+-------------+---------+--------------+-------------+----------------+-------------------+------------------+---------------------+---------------------+------------+-------------------+--------------------+--------------------+--------------------+--------------+--------------------+----------------+--------------------+--------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.sql(\"select * from c2c_copy\")\n",
    "print(df.count())\n",
    "df.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+----------------+-------------------+--------------------+--------------------+--------------------+------+---------------+-------------+---------+--------------+-------------+----------------+-------------------+------------------+---------------------+---------------------+------------+-------------------+--------------------+--------------------+--------------------+--------------+--------------------+----------------+--------------------+--------------+--------------------+\n",
      "|                 AID|        OWNER_USERID|OWNER_CELLNUMBER|             AMOUNT|        CREATIONDATE|      EXPIRATIONDATE|        EXERCISEDATE|STATUS|      FEECHARGE|         TYPE|INITIATOR|SOURCE_POSTFIX|SOURCE_PREFIX|SOURCE_BANK_CODE|DESTINATION_POSTFIX|DESTINATION_PREFIX|DESTINATION_BANK_CODE|DESTINATION_CARDINDEX|         RRN|              GDATE|                  IP|        TRACKINGCODE|      creationdate_d|creationdate_j|    expirationdate_d|expirationdate_j|      exercisedate_d|exercisedate_j|diff_exercise_create|\n",
      "+--------------------+--------------------+----------------+-------------------+--------------------+--------------------+--------------------+------+---------------+-------------+---------+--------------+-------------+----------------+-------------------+------------------+---------------------+---------------------+------------+-------------------+--------------------+--------------------+--------------------+--------------+--------------------+----------------+--------------------+--------------+--------------------+\n",
      "|60565c55629149000...|7395dd6c-cae3-441...|     09126494166|13000000.0000000000|1616272469385.000...|1616358869385.000...|1616272474997.000...|     8|8400.0000000000|CARD_TRANSFER|   MOBILE|          6195|       504706|             061|               9355|            603770|                  016|                 null|        null|2021-03-21 00:00:00|2.147.59.5, 2.147...|16968857901616272...|+53187-08-21 11:1...|          null|+53190-05-17 11:1...|            null|+53187-08-21 12:5...|          null|      5612.000000000|\n",
      "|60565cc084a54c000...|6bacd64a-d9ab-499...|     09308118835| 2700000.0000000000|1616272576818.000...|1616358976818.000...|1616272578914.000...|     8|          0E-10|CARD_TRANSFER|   MOBILE|          1496|       622106|             054|               9036|            622106|                  054|                 null|       61351|2021-03-21 00:00:00|151.234.14.137, 1...|77383579016162725...|+53187-08-22 17:1...|          null|+53190-05-18 17:1...|            null|+53187-08-22 17:4...|          null|      2096.000000000|\n",
      "|60565cc57b3387000...|fe591dcd-2bac-424...|     09125607421| 2500000.0000000000|1616272581342.000...|1616358981342.000...|1616272584558.000...|     8|6000.0000000000|CARD_TRANSFER|   MOBILE|          5929|       585983|             018|               7305|            610433|                  012|                 null|        null|2021-03-21 00:00:00|89.38.102.5, 89.3...|59324525616162725...|+53187-08-22 18:2...|          null|+53190-05-18 18:2...|            null|+53187-08-22 19:1...|          null|      3216.000000000|\n",
      "|605772f8629149000...|b0837814-256d-4b5...|     09197298580| 2000000.0000000000|1616343800700.000...|1616430200700.000...|1616343804162.000...|     8|6000.0000000000|CARD_TRANSFER|   MOBILE|          1156|       502229|             057|               0622|            610433|                  012|                 null| 27309890255|2021-03-21 00:00:00|86.55.161.87, 86....|16790535681616343...|+53189-11-24 00:3...|          null|+53192-08-20 01:3...|            null|+53189-11-24 01:3...|          null|      3462.000000000|\n",
      "|605772f9629149000...|9a4f5456-84d6-484...|     09172703585|  460200.0000000000|1616343801497.000...|1616430201497.000...|1616343804527.000...|     8|6000.0000000000|CARD_TRANSFER|   MOBILE|          8528|       585983|             018|               1778|            627381|                  063|                 null|        null|2021-03-21 00:00:00|5.134.149.54, 5.1...|19614581611616343...|+53189-11-24 00:4...|          null|+53192-08-20 01:4...|            null|+53189-11-24 01:3...|          null|      3030.000000000|\n",
      "|605773007b3387000...|266c620a-5ded-411...|     09028589356| 5000000.0000000000|1616343808764.000...|1616430208764.000...|1616343810601.000...|     8|6000.0000000000|CARD_TRANSFER|   MOBILE|          9266|       610433|             012|               4203|            628023|                  014|                 null|107843220774|2021-03-21 00:00:00|95.162.155.10, 95...|11586064591616343...|+53189-11-24 02:4...|          null|+53192-08-20 03:4...|            null|+53189-11-24 03:2...|          null|      1837.000000000|\n",
      "|60577305629149000...|4b09ff23-805b-42e...|     09373737339| 3800000.0000000000|1616343813031.000...|1616430213031.000...|1616343815543.000...|     8|6000.0000000000|CARD_TRANSFER|   MOBILE|          6637|       622106|             054|               2347|            603799|                  017|                 null|      445351|2021-03-21 00:00:00|89.45.126.144, 89...|14870134591616343...|+53189-11-24 04:0...|          null|+53192-08-20 05:0...|            null|+53189-11-24 04:4...|          null|      2512.000000000|\n",
      "|60565d047b3387000...|ad7ab34e-e00c-42e...|     09195560293|10000000.0000000000|1616272644724.000...|1616359044724.000...|1616272651339.000...|     8|6000.0000000000|CARD_TRANSFER|   MOBILE|          4952|       504706|             061|               7459|            628023|                  014|                 null|        null|2021-03-21 00:00:00|89.196.73.168, 89...|65216752016162726...|+53187-08-23 12:0...|          null|+53190-05-19 12:0...|            null|+53187-08-23 13:5...|          null|      6615.000000000|\n",
      "|60565d10629149000...|4ba2af4a-a658-41c...|     09187109687| 2600000.0000000000|1616272656581.000...|1616359056581.000...|1616272662507.000...|     8|6000.0000000000|CARD_TRANSFER|   MOBILE|          6889|       636949|             065|               2629|            610433|                  012|                 null|108000745003|2021-03-21 00:00:00|5.124.54.251, 5.1...|19861407191616272...|+53187-08-23 15:1...|          null|+53190-05-19 15:1...|            null|+53187-08-23 16:5...|          null|      5926.000000000|\n",
      "|60565d2f629149000...|6bacd64a-d9ab-499...|     09308118835| 3800000.0000000000|1616272687310.000...|1616359087310.000...|1616272689969.000...|     8|6000.0000000000|CARD_TRANSFER|   MOBILE|          1496|       622106|             054|               0024|            628023|                  014|                 null|       61367|2021-03-21 00:00:00|151.234.14.137, 1...|28768904916162726...|+53187-08-23 23:5...|          null|+53190-05-19 23:5...|            null|+53187-08-24 00:3...|          null|      2659.000000000|\n",
      "|60565d65629149000...|ea52ac93-76ea-48d...|     09910933197|10000000.0000000000|1616272741350.000...|1616359141350.000...|1616272745852.000...|     8|6000.0000000000|CARD_TRANSFER|   MOBILE|          9999|       627381|             063|               8868|            589210|                  015|                 null|130004998136|2021-03-21 00:00:00|83.120.67.197, 83...|11047406131616272...|+53187-08-24 14:5...|          null|+53190-05-20 14:5...|            null|+53187-08-24 16:0...|          null|      4502.000000000|\n",
      "|60565ddd629149000...|5ae65f46-ef52-4ae...|     09035093668|  900000.0000000000|1616272861613.000...|1616359261613.000...|1616272874169.000...|     8|6000.0000000000|CARD_TRANSFER|   MOBILE|          4819|       504706|             061|               9311|            589210|                  015|                 null|        null|2021-03-21 00:00:00|5.115.104.45, 5.1...|10553971611616272...|+53187-08-26 00:1...|          null|+53190-05-22 00:1...|            null|+53187-08-26 03:4...|          null|     12556.000000000|\n",
      "|60565df484a54c000...|34b6b677-ee7d-474...|     09120649010| 3000000.0000000000|1616272884324.000...|1616359284324.000...|1616272887272.000...|     8|          0E-10|CARD_TRANSFER|   MOBILE|          6374|       502229|             057|               2548|            502229|                  057|                 null| 27300654810|2021-03-21 00:00:00|188.229.10.195, 1...|17214583991616272...|+53187-08-26 06:3...|          null|+53190-05-22 06:3...|            null|+53187-08-26 07:2...|          null|      2948.000000000|\n",
      "|60565e0384a54c000...|9e6cce2a-0e4e-4dd...|     09211781720|  200000.0000000000|1616272899352.000...|1616359299352.000...|1616272902495.000...|     8|6000.0000000000|CARD_TRANSFER|   MOBILE|          3649|       504706|             061|               9364|            505416|                  064|                 null|        null|2021-03-21 00:00:00|185.8.172.200, 18...|10651491211616272...|+53187-08-26 10:4...|          null|+53190-05-22 10:4...|            null|+53187-08-26 11:3...|          null|      3143.000000000|\n",
      "|60565dfc629149000...|6bfeafdf-3ffc-45c...|     09126101191|10000000.0000000000|1616272892579.000...|1616359292579.000...|1616272902721.000...|     8|6000.0000000000|CARD_TRANSFER|   MOBILE|          8121|       589210|             015|               8294|            636949|                  065|                 null|     8202971|2021-03-21 00:00:00|5.114.58.164, 5.1...|14582985371616272...|+53187-08-26 08:5...|          null|+53190-05-22 08:5...|            null|+53187-08-26 11:4...|          null|     10142.000000000|\n",
      "|60577a0c7b3387000...|32c68834-c81c-47a...|     09109122258|13380000.0000000000|1616345612364.000...|1616432012364.000...|1616345615931.000...|     8|8400.0000000000|CARD_TRANSFER|   MOBILE|          3246|       507677|             080|               6219|            639599|                  052|                 null|222023337469|2021-03-21 00:00:00|5.215.153.202, 5....|87094191516163456...|+53189-12-14 23:4...|          null|+53192-09-10 00:4...|            null|+53189-12-15 00:4...|          null|      3567.000000000|\n",
      "|60577a1a84a54c000...|57771f75-43d6-4cc...|     09907299010|  500000.0000000000|1616345626255.000...|1616432026255.000...|1616345628308.000...|     8|          0E-10|CARD_TRANSFER|   MOBILE|          0227|       585983|             018|               0050|            585983|                  018|                 null|        null|2021-03-21 00:00:00|194.5.178.165, 19...|34082864116163456...|+53189-12-15 03:4...|          null|+53192-09-10 04:4...|            null|+53189-12-15 04:1...|          null|      2053.000000000|\n",
      "|60577a2584a54c000...|c46b79e2-b61e-4a7...|     09335193889| 1000000.0000000000|1616345637510.000...|1616432037510.000...|1616345641128.000...|     8|6000.0000000000|CARD_TRANSFER|   MOBILE|          1679|       585983|             018|               5668|            502229|                  057|                 null|        null|2021-03-21 00:00:00|46.167.131.97, 46...|37678399161634563...|+53189-12-15 06:4...|          null|+53192-09-10 07:4...|            null|+53189-12-15 07:4...|          null|      3618.000000000|\n",
      "|60577a29629149000...|d11242c7-9155-4a1...|     09126488509|10000000.0000000000|1616345641985.000...|1616432041985.000...|1616345643842.000...|     8|6000.0000000000|CARD_TRANSFER|   MOBILE|          0957|       610433|             012|               4997|            504706|                  061|                 null|107844018508|2021-03-21 00:00:00|89.45.61.82, 89.4...|10791493301616345...|+53189-12-15 08:0...|          null|+53192-09-10 09:0...|            null|+53189-12-15 08:3...|          null|      1857.000000000|\n",
      "|60577a2d629149000...|8cabee35-ab16-4e0...|     09381201370|  100000.0000000000|1616345645006.000...|1616432045006.000...|1616345647992.000...|     8|          0E-10|CARD_TRANSFER|   MOBILE|          4645|       627381|             063|               4421|            627381|                  063|                 null|130006292611|2021-03-21 00:00:00|5.126.121.38, 5.1...|94921200816163456...|+53189-12-15 08:5...|          null|+53192-09-10 09:5...|            null|+53189-12-15 09:4...|          null|      2986.000000000|\n",
      "+--------------------+--------------------+----------------+-------------------+--------------------+--------------------+--------------------+------+---------------+-------------+---------+--------------+-------------+----------------+-------------------+------------------+---------------------+---------------------+------------+-------------------+--------------------+--------------------+--------------------+--------------+--------------------+----------------+--------------------+--------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.sql(\"select * from parquet.c2c\")\n",
    "# df_bet = spark.sql(\"select * from parquet.c2c_bet\")\n",
    "df.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+----------------+---------------+------------+----------+------------+------------+---------+----------+----------+-----------+--------------------+-------+----------+-------------+---+------------+------------+\n",
      "|         pan_src|        pan_dest|transactiontype|terminaltype|      date|        time|         rrn|   amount|merchantno|terminalno|bank_issuer|                 psp|channel|mobile_app|   cellnumber|url|channel_date|channel_time|\n",
      "+----------------+----------------+---------------+------------+----------+------------+------------+---------+----------+----------+-----------+--------------------+-------+----------+-------------+---+------------+------------+\n",
      "|6037998141056563|5892101335946055|     |          59|1400/08/24|15:48:14.346|852959131941|3000000.0| 999999556|  17592030|        |  ...| |      |9.370229035E9|NaN|         NaN|         NaN|\n",
      "|6037998170196009|5894631161088893|     |          59|1400/08/10|10:58:46.283|547854278219|1000000.0| 999999556|  17592030|        |  ...| |      |9.931847345E9|NaN|         NaN|         NaN|\n",
      "|6037998132669275|5894631160654760|     |          59|1400/08/03|05:33:30.465|749349710040|1000000.0| 999999556|  17592030|        |  ...| |      |9.168993609E9|NaN|         NaN|         NaN|\n",
      "|6037998143282514|5859831193485017|     |          59|1400/08/22|21:27:18.944|680339939852|1000000.0| 999999556|  17592030|        |  ...| |      |9.378547323E9|NaN|         NaN|         NaN|\n",
      "|6037998142687606|5894631160667986|     |          59|1400/08/07|19:15:18.670|776344618029|1000000.0| 999999556|  17592030|        |  ...| |      |9.034882912E9|NaN|         NaN|         NaN|\n",
      "|6037998133693753|5894631160072187|     |          59|1400/08/04|16:04:21.035|476463735758|1000000.0| 999999556|  17592030|        |  ...| |      |9.111850871E9|NaN|         NaN|         NaN|\n",
      "|6037998149314220|5894631160654760|     |          59|1400/08/03|04:32:51.576|912558774061|1000000.0| 999999556|  17592030|        |  ...| |      |9.156290052E9|NaN|         NaN|         NaN|\n",
      "|6037998172834664|5894631160072187|     |          59|1400/08/03|12:12:40.615|738337301317|    1.1E7| 999999556|  17592030|        |  ...| |      |9.189828548E9|NaN|         NaN|         NaN|\n",
      "|6037998173010678|5894631160682928|     |          59|1400/08/03|00:45:39.442|443245550208|1000000.0| 999999556|  17592030|        |  ...| |      |9.902959415E9|NaN|         NaN|         NaN|\n",
      "|6037998177729968|5894631161078704|     |          59|1400/08/05|07:55:28.057|753621710212|    1.0E7| 999999556|  17592030|        |  ...| |      |9.114796738E9|NaN|         NaN|         NaN|\n",
      "|6037998176598166|5022291306890284|     |          59|1400/08/23|01:14:19.545|477866518241|4000000.0| 999999556|  17592030|        |  ...| |      |9.905611625E9|NaN|         NaN|         NaN|\n",
      "|6037998190647437|5894631160072187|     |          59|1400/08/03|13:15:26.458|829243833230|    1.0E7| 999999556|  17592030|        |  ...| |      |9.176362769E9|NaN|         NaN|         NaN|\n",
      "|6037998171488397|5894631161057328|     |          59|1400/08/04|11:27:26.563|131575162645|1000000.0| 999999556|  17592030|        |  ...| |      |9.146499823E9|NaN|         NaN|         NaN|\n",
      "|6037998171488397|5894631161078704|     |          59|1400/08/06|00:58:33.372|544270082055|1000000.0| 999999556|  17592030|        |  ...| |      |9.146499823E9|NaN|         NaN|         NaN|\n",
      "|6037998171488397|5894631161078704|     |          59|1400/08/06|01:04:31.523|183763571090|1000000.0| 999999556|  17592030|        |  ...| |      |9.146499823E9|NaN|         NaN|         NaN|\n",
      "|6037998171488397|5894631161078704|     |          59|1400/08/06|01:15:50.038|737204561051|2000000.0| 999999556|  17592030|        |  ...| |      |9.146499823E9|NaN|         NaN|         NaN|\n",
      "|6037998190715747|5894631161916721|     |          59|1400/08/26|00:15:26.956|141963774066|1000000.0| 999999556|  17592030|        |  ...| |      |9.935844811E9|NaN|         NaN|         NaN|\n",
      "|6037998196617160|5894631161078704|     |          59|1400/08/06|01:00:08.754|629135941035|3000000.0| 999999556|  17592030|        |  ...| |      |9.190953119E9|NaN|         NaN|         NaN|\n",
      "|6037998196617160|5894631161078704|     |          59|1400/08/06|01:22:19.003|177954843436|1000000.0| 999999556|  17592030|        |  ...| |      |9.190953119E9|NaN|         NaN|         NaN|\n",
      "|6037998109292093|5894631161173216|     |          59|1400/08/06|09:02:42.042|574873751762|2000000.0| 999999556|  17592030|        |  ...| |      |9.903071469E9|NaN|         NaN|         NaN|\n",
      "+----------------+----------------+---------------+------------+----------+------------+------------+---------+----------+----------+-----------+--------------------+-------+----------+-------------+---+------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_bet = spark.sql(\"select * from parquet.c2c_bet\")\n",
    "# df_bet = spark.sql(\"select * from parquet.c2c_bet\")\n",
    "df_bet.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [],
   "source": [
    "# Expected: string, Found: DOUBLE\n",
    "# spark.conf.set(\"spark.sql.parquet.enableVectorizedReader\",\"false\")\n",
    "spark.conf.set(\"spark.sql.parquet.enableVectorizedReader\",\"false\")\n",
    "df_bet=df_bet.withColumn(\"channel\",functions.concat_ws(\" \",df_bet[\"channel\"],df_bet[\"channel\"]))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "outputs": [],
   "source": [
    "# df_bet.groupby(\"channel\").count().show() # error\n",
    "df_bet=df_bet.drop(\"channel\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o483.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 7 in stage 32.0 failed 1 times, most recent failure: Lost task 7.0 in stage 32.0 (TID 151) (host.docker.internal executor driver): org.apache.spark.sql.execution.QueryExecutionException: Encounter error while reading parquet files. One possible cause: Parquet column cannot be converted in the corresponding files. Details: \r\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.cannotReadParquetFilesError(QueryExecutionErrors.scala:577)\r\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:200)\r\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:104)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.agg_doAggregateWithKeys_0$(Unknown Source)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\r\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630)\r\n\tat java.base/java.lang.Thread.run(Thread.java:832)\r\nCaused by: org.apache.parquet.io.ParquetDecodingException: Can not read value at 1 in block 0 in file file:/D:/workspace/digipay-data-science/c2c-bet-fraud/c2cbet/c2c_bet/part-00003-c295f7b9-63fd-4f7d-b3da-9fb623fa77cc-c000.snappy.parquet\r\n\tat org.apache.parquet.hadoop.InternalParquetRecordReader.nextKeyValue(InternalParquetRecordReader.java:254)\r\n\tat org.apache.parquet.hadoop.ParquetRecordReader.nextKeyValue(ParquetRecordReader.java:207)\r\n\tat org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)\r\n\tat org.apache.spark.sql.execution.datasources.RecordReaderIterator$$anon$1.hasNext(RecordReaderIterator.scala:61)\r\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:104)\r\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:191)\r\n\t... 18 more\r\nCaused by: java.lang.ClassCastException: class [B cannot be cast to class java.lang.Long ([B and java.lang.Long are in module java.base of loader 'bootstrap')\r\n\tat scala.runtime.BoxesRunTime.unboxToLong(BoxesRunTime.java:107)\r\n\tat org.apache.spark.sql.catalyst.expressions.MutableLong.update(SpecificInternalRow.scala:148)\r\n\tat org.apache.spark.sql.catalyst.expressions.SpecificInternalRow.update(SpecificInternalRow.scala:249)\r\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetRowConverter$RowUpdater.set(ParquetRowConverter.scala:175)\r\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetPrimitiveConverter.addBinary(ParquetRowConverter.scala:93)\r\n\tat org.apache.parquet.column.impl.ColumnReaderBase$2$6.writeValue(ColumnReaderBase.java:390)\r\n\tat org.apache.parquet.column.impl.ColumnReaderBase.writeCurrentValueToConverter(ColumnReaderBase.java:440)\r\n\tat org.apache.parquet.column.impl.ColumnReaderImpl.writeCurrentValueToConverter(ColumnReaderImpl.java:30)\r\n\tat org.apache.parquet.io.RecordReaderImplementation.read(RecordReaderImplementation.java:406)\r\n\tat org.apache.parquet.hadoop.InternalParquetRecordReader.nextKeyValue(InternalParquetRecordReader.java:229)\r\n\t... 23 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2454)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2403)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2402)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2402)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1160)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1160)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1160)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2642)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2584)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2573)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\nCaused by: org.apache.spark.sql.execution.QueryExecutionException: Encounter error while reading parquet files. One possible cause: Parquet column cannot be converted in the corresponding files. Details: \r\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.cannotReadParquetFilesError(QueryExecutionErrors.scala:577)\r\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:200)\r\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:104)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.agg_doAggregateWithKeys_0$(Unknown Source)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\r\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630)\r\n\tat java.base/java.lang.Thread.run(Thread.java:832)\r\nCaused by: org.apache.parquet.io.ParquetDecodingException: Can not read value at 1 in block 0 in file file:/D:/workspace/digipay-data-science/c2c-bet-fraud/c2cbet/c2c_bet/part-00003-c295f7b9-63fd-4f7d-b3da-9fb623fa77cc-c000.snappy.parquet\r\n\tat org.apache.parquet.hadoop.InternalParquetRecordReader.nextKeyValue(InternalParquetRecordReader.java:254)\r\n\tat org.apache.parquet.hadoop.ParquetRecordReader.nextKeyValue(ParquetRecordReader.java:207)\r\n\tat org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)\r\n\tat org.apache.spark.sql.execution.datasources.RecordReaderIterator$$anon$1.hasNext(RecordReaderIterator.scala:61)\r\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:104)\r\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:191)\r\n\t... 18 more\r\nCaused by: java.lang.ClassCastException: class [B cannot be cast to class java.lang.Long ([B and java.lang.Long are in module java.base of loader 'bootstrap')\r\n\tat scala.runtime.BoxesRunTime.unboxToLong(BoxesRunTime.java:107)\r\n\tat org.apache.spark.sql.catalyst.expressions.MutableLong.update(SpecificInternalRow.scala:148)\r\n\tat org.apache.spark.sql.catalyst.expressions.SpecificInternalRow.update(SpecificInternalRow.scala:249)\r\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetRowConverter$RowUpdater.set(ParquetRowConverter.scala:175)\r\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetPrimitiveConverter.addBinary(ParquetRowConverter.scala:93)\r\n\tat org.apache.parquet.column.impl.ColumnReaderBase$2$6.writeValue(ColumnReaderBase.java:390)\r\n\tat org.apache.parquet.column.impl.ColumnReaderBase.writeCurrentValueToConverter(ColumnReaderBase.java:440)\r\n\tat org.apache.parquet.column.impl.ColumnReaderImpl.writeCurrentValueToConverter(ColumnReaderImpl.java:30)\r\n\tat org.apache.parquet.io.RecordReaderImplementation.read(RecordReaderImplementation.java:406)\r\n\tat org.apache.parquet.hadoop.InternalParquetRecordReader.nextKeyValue(InternalParquetRecordReader.java:229)\r\n\t... 23 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mPy4JJavaError\u001B[0m                             Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp\\ipykernel_19988\\383240905.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[1;32m----> 1\u001B[1;33m \u001B[0mdf_bet\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mgroupby\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m\"pan_src\"\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mcount\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mshow\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;31m# error\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      2\u001B[0m \u001B[1;31m# pan_src|        pan_dest|transactiontype|terminaltype|      date|        time|         rrn|   amount|merchantno|terminalno|bank_issuer|                 psp|channel|mobile_app|   cellnumber|url|channel_date|channel_time\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      3\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\app\\python38\\lib\\site-packages\\pyspark\\sql\\dataframe.py\u001B[0m in \u001B[0;36mshow\u001B[1;34m(self, n, truncate, vertical)\u001B[0m\n\u001B[0;32m    492\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    493\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[0misinstance\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtruncate\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mbool\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;32mand\u001B[0m \u001B[0mtruncate\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 494\u001B[1;33m             \u001B[0mprint\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_jdf\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mshowString\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mn\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;36m20\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mvertical\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    495\u001B[0m         \u001B[1;32melse\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    496\u001B[0m             \u001B[1;32mtry\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\app\\python38\\lib\\site-packages\\py4j\\java_gateway.py\u001B[0m in \u001B[0;36m__call__\u001B[1;34m(self, *args)\u001B[0m\n\u001B[0;32m   1319\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1320\u001B[0m         \u001B[0manswer\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mgateway_client\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msend_command\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mcommand\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1321\u001B[1;33m         return_value = get_return_value(\n\u001B[0m\u001B[0;32m   1322\u001B[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001B[0;32m   1323\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\app\\python38\\lib\\site-packages\\pyspark\\sql\\utils.py\u001B[0m in \u001B[0;36mdeco\u001B[1;34m(*a, **kw)\u001B[0m\n\u001B[0;32m    109\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0mdeco\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0ma\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkw\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    110\u001B[0m         \u001B[1;32mtry\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 111\u001B[1;33m             \u001B[1;32mreturn\u001B[0m \u001B[0mf\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0ma\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkw\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    112\u001B[0m         \u001B[1;32mexcept\u001B[0m \u001B[0mpy4j\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mprotocol\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mPy4JJavaError\u001B[0m \u001B[1;32mas\u001B[0m \u001B[0me\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    113\u001B[0m             \u001B[0mconverted\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mconvert_exception\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0me\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mjava_exception\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\app\\python38\\lib\\site-packages\\py4j\\protocol.py\u001B[0m in \u001B[0;36mget_return_value\u001B[1;34m(answer, gateway_client, target_id, name)\u001B[0m\n\u001B[0;32m    324\u001B[0m             \u001B[0mvalue\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mOUTPUT_CONVERTER\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mtype\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0manswer\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m2\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mgateway_client\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    325\u001B[0m             \u001B[1;32mif\u001B[0m \u001B[0manswer\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m]\u001B[0m \u001B[1;33m==\u001B[0m \u001B[0mREFERENCE_TYPE\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 326\u001B[1;33m                 raise Py4JJavaError(\n\u001B[0m\u001B[0;32m    327\u001B[0m                     \u001B[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001B[0m\u001B[1;33m.\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    328\u001B[0m                     format(target_id, \".\", name), value)\n",
      "\u001B[1;31mPy4JJavaError\u001B[0m: An error occurred while calling o483.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 7 in stage 32.0 failed 1 times, most recent failure: Lost task 7.0 in stage 32.0 (TID 151) (host.docker.internal executor driver): org.apache.spark.sql.execution.QueryExecutionException: Encounter error while reading parquet files. One possible cause: Parquet column cannot be converted in the corresponding files. Details: \r\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.cannotReadParquetFilesError(QueryExecutionErrors.scala:577)\r\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:200)\r\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:104)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.agg_doAggregateWithKeys_0$(Unknown Source)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\r\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630)\r\n\tat java.base/java.lang.Thread.run(Thread.java:832)\r\nCaused by: org.apache.parquet.io.ParquetDecodingException: Can not read value at 1 in block 0 in file file:/D:/workspace/digipay-data-science/c2c-bet-fraud/c2cbet/c2c_bet/part-00003-c295f7b9-63fd-4f7d-b3da-9fb623fa77cc-c000.snappy.parquet\r\n\tat org.apache.parquet.hadoop.InternalParquetRecordReader.nextKeyValue(InternalParquetRecordReader.java:254)\r\n\tat org.apache.parquet.hadoop.ParquetRecordReader.nextKeyValue(ParquetRecordReader.java:207)\r\n\tat org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)\r\n\tat org.apache.spark.sql.execution.datasources.RecordReaderIterator$$anon$1.hasNext(RecordReaderIterator.scala:61)\r\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:104)\r\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:191)\r\n\t... 18 more\r\nCaused by: java.lang.ClassCastException: class [B cannot be cast to class java.lang.Long ([B and java.lang.Long are in module java.base of loader 'bootstrap')\r\n\tat scala.runtime.BoxesRunTime.unboxToLong(BoxesRunTime.java:107)\r\n\tat org.apache.spark.sql.catalyst.expressions.MutableLong.update(SpecificInternalRow.scala:148)\r\n\tat org.apache.spark.sql.catalyst.expressions.SpecificInternalRow.update(SpecificInternalRow.scala:249)\r\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetRowConverter$RowUpdater.set(ParquetRowConverter.scala:175)\r\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetPrimitiveConverter.addBinary(ParquetRowConverter.scala:93)\r\n\tat org.apache.parquet.column.impl.ColumnReaderBase$2$6.writeValue(ColumnReaderBase.java:390)\r\n\tat org.apache.parquet.column.impl.ColumnReaderBase.writeCurrentValueToConverter(ColumnReaderBase.java:440)\r\n\tat org.apache.parquet.column.impl.ColumnReaderImpl.writeCurrentValueToConverter(ColumnReaderImpl.java:30)\r\n\tat org.apache.parquet.io.RecordReaderImplementation.read(RecordReaderImplementation.java:406)\r\n\tat org.apache.parquet.hadoop.InternalParquetRecordReader.nextKeyValue(InternalParquetRecordReader.java:229)\r\n\t... 23 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2454)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2403)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2402)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2402)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1160)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1160)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1160)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2642)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2584)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2573)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\nCaused by: org.apache.spark.sql.execution.QueryExecutionException: Encounter error while reading parquet files. One possible cause: Parquet column cannot be converted in the corresponding files. Details: \r\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.cannotReadParquetFilesError(QueryExecutionErrors.scala:577)\r\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:200)\r\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:104)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.agg_doAggregateWithKeys_0$(Unknown Source)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\r\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630)\r\n\tat java.base/java.lang.Thread.run(Thread.java:832)\r\nCaused by: org.apache.parquet.io.ParquetDecodingException: Can not read value at 1 in block 0 in file file:/D:/workspace/digipay-data-science/c2c-bet-fraud/c2cbet/c2c_bet/part-00003-c295f7b9-63fd-4f7d-b3da-9fb623fa77cc-c000.snappy.parquet\r\n\tat org.apache.parquet.hadoop.InternalParquetRecordReader.nextKeyValue(InternalParquetRecordReader.java:254)\r\n\tat org.apache.parquet.hadoop.ParquetRecordReader.nextKeyValue(ParquetRecordReader.java:207)\r\n\tat org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)\r\n\tat org.apache.spark.sql.execution.datasources.RecordReaderIterator$$anon$1.hasNext(RecordReaderIterator.scala:61)\r\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:104)\r\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:191)\r\n\t... 18 more\r\nCaused by: java.lang.ClassCastException: class [B cannot be cast to class java.lang.Long ([B and java.lang.Long are in module java.base of loader 'bootstrap')\r\n\tat scala.runtime.BoxesRunTime.unboxToLong(BoxesRunTime.java:107)\r\n\tat org.apache.spark.sql.catalyst.expressions.MutableLong.update(SpecificInternalRow.scala:148)\r\n\tat org.apache.spark.sql.catalyst.expressions.SpecificInternalRow.update(SpecificInternalRow.scala:249)\r\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetRowConverter$RowUpdater.set(ParquetRowConverter.scala:175)\r\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetPrimitiveConverter.addBinary(ParquetRowConverter.scala:93)\r\n\tat org.apache.parquet.column.impl.ColumnReaderBase$2$6.writeValue(ColumnReaderBase.java:390)\r\n\tat org.apache.parquet.column.impl.ColumnReaderBase.writeCurrentValueToConverter(ColumnReaderBase.java:440)\r\n\tat org.apache.parquet.column.impl.ColumnReaderImpl.writeCurrentValueToConverter(ColumnReaderImpl.java:30)\r\n\tat org.apache.parquet.io.RecordReaderImplementation.read(RecordReaderImplementation.java:406)\r\n\tat org.apache.parquet.hadoop.InternalParquetRecordReader.nextKeyValue(InternalParquetRecordReader.java:229)\r\n\t... 23 more\r\n"
     ]
    }
   ],
   "source": [
    "df_bet.groupby(\"pan_src\").count().show() # error\n",
    "# pan_src|        pan_dest|transactiontype|terminaltype|      date|        time|         rrn|   amount|merchantno|terminalno|bank_issuer|                 psp|channel|mobile_app|   cellnumber|url|channel_date|channel_time"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o469.saveAsTable.\n: org.apache.spark.SparkException: Job aborted.\r\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.jobAbortedError(QueryExecutionErrors.scala:496)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:251)\r\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:186)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource.writeAndRead(DataSource.scala:537)\r\n\tat org.apache.spark.sql.execution.command.CreateDataSourceTableAsSelectCommand.saveDataIntoTable(createDataSourceTables.scala:228)\r\n\tat org.apache.spark.sql.execution.command.CreateDataSourceTableAsSelectCommand.run(createDataSourceTables.scala:182)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:110)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:110)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:106)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:481)\r\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:82)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:481)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:457)\r\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:106)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:93)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:91)\r\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:128)\r\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:848)\r\n\tat org.apache.spark.sql.DataFrameWriter.createTable(DataFrameWriter.scala:689)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveAsTable(DataFrameWriter.scala:667)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveAsTable(DataFrameWriter.scala:565)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:564)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:832)\r\nCaused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 7 in stage 31.0 failed 1 times, most recent failure: Lost task 7.0 in stage 31.0 (TID 143) (host.docker.internal executor driver): org.apache.spark.sql.execution.QueryExecutionException: Encounter error while reading parquet files. One possible cause: Parquet column cannot be converted in the corresponding files. Details: \r\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.cannotReadParquetFilesError(QueryExecutionErrors.scala:577)\r\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:200)\r\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:104)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:286)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$16(FileFormatWriter.scala:229)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630)\r\n\tat java.base/java.lang.Thread.run(Thread.java:832)\r\nCaused by: org.apache.parquet.io.ParquetDecodingException: Can not read value at 0 in block -1 in file file:/D:/workspace/digipay-data-science/c2c-bet-fraud/c2cbet/c2c_bet/part-00003-c295f7b9-63fd-4f7d-b3da-9fb623fa77cc-c000.snappy.parquet\r\n\tat org.apache.parquet.hadoop.InternalParquetRecordReader.nextKeyValue(InternalParquetRecordReader.java:254)\r\n\tat org.apache.parquet.hadoop.ParquetRecordReader.nextKeyValue(ParquetRecordReader.java:207)\r\n\tat org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)\r\n\tat org.apache.spark.sql.execution.datasources.RecordReaderIterator$$anon$1.hasNext(RecordReaderIterator.scala:61)\r\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:104)\r\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:191)\r\n\t... 12 more\r\nCaused by: java.lang.UnsupportedOperationException: org.apache.parquet.column.values.dictionary.PlainValuesDictionary$PlainDoubleDictionary\r\n\tat org.apache.parquet.column.Dictionary.decodeToBinary(Dictionary.java:41)\r\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetRowConverter$ParquetStringConverter.$anonfun$setDictionary$1(ParquetRowConverter.scala:449)\r\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetRowConverter$ParquetStringConverter.$anonfun$setDictionary$1$adapted(ParquetRowConverter.scala:448)\r\n\tat scala.Array$.tabulate(Array.scala:418)\r\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetRowConverter$ParquetStringConverter.setDictionary(ParquetRowConverter.scala:448)\r\n\tat org.apache.parquet.column.impl.ColumnReaderBase.<init>(ColumnReaderBase.java:415)\r\n\tat org.apache.parquet.column.impl.ColumnReaderImpl.<init>(ColumnReaderImpl.java:46)\r\n\tat org.apache.parquet.column.impl.ColumnReadStoreImpl.getColumnReader(ColumnReadStoreImpl.java:82)\r\n\tat org.apache.parquet.io.RecordReaderImplementation.<init>(RecordReaderImplementation.java:271)\r\n\tat org.apache.parquet.io.MessageColumnIO$1.visit(MessageColumnIO.java:147)\r\n\tat org.apache.parquet.io.MessageColumnIO$1.visit(MessageColumnIO.java:109)\r\n\tat org.apache.parquet.filter2.compat.FilterCompat$NoOpFilter.accept(FilterCompat.java:177)\r\n\tat org.apache.parquet.io.MessageColumnIO.getRecordReader(MessageColumnIO.java:109)\r\n\tat org.apache.parquet.hadoop.InternalParquetRecordReader.checkRead(InternalParquetRecordReader.java:136)\r\n\tat org.apache.parquet.hadoop.InternalParquetRecordReader.nextKeyValue(InternalParquetRecordReader.java:225)\r\n\t... 17 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2454)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2403)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2402)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2402)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1160)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1160)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1160)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2642)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2584)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2573)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:938)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2214)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:218)\r\n\t... 44 more\r\nCaused by: org.apache.spark.sql.execution.QueryExecutionException: Encounter error while reading parquet files. One possible cause: Parquet column cannot be converted in the corresponding files. Details: \r\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.cannotReadParquetFilesError(QueryExecutionErrors.scala:577)\r\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:200)\r\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:104)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:286)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$16(FileFormatWriter.scala:229)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630)\r\n\t... 1 more\r\nCaused by: org.apache.parquet.io.ParquetDecodingException: Can not read value at 0 in block -1 in file file:/D:/workspace/digipay-data-science/c2c-bet-fraud/c2cbet/c2c_bet/part-00003-c295f7b9-63fd-4f7d-b3da-9fb623fa77cc-c000.snappy.parquet\r\n\tat org.apache.parquet.hadoop.InternalParquetRecordReader.nextKeyValue(InternalParquetRecordReader.java:254)\r\n\tat org.apache.parquet.hadoop.ParquetRecordReader.nextKeyValue(ParquetRecordReader.java:207)\r\n\tat org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)\r\n\tat org.apache.spark.sql.execution.datasources.RecordReaderIterator$$anon$1.hasNext(RecordReaderIterator.scala:61)\r\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:104)\r\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:191)\r\n\t... 12 more\r\nCaused by: java.lang.UnsupportedOperationException: org.apache.parquet.column.values.dictionary.PlainValuesDictionary$PlainDoubleDictionary\r\n\tat org.apache.parquet.column.Dictionary.decodeToBinary(Dictionary.java:41)\r\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetRowConverter$ParquetStringConverter.$anonfun$setDictionary$1(ParquetRowConverter.scala:449)\r\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetRowConverter$ParquetStringConverter.$anonfun$setDictionary$1$adapted(ParquetRowConverter.scala:448)\r\n\tat scala.Array$.tabulate(Array.scala:418)\r\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetRowConverter$ParquetStringConverter.setDictionary(ParquetRowConverter.scala:448)\r\n\tat org.apache.parquet.column.impl.ColumnReaderBase.<init>(ColumnReaderBase.java:415)\r\n\tat org.apache.parquet.column.impl.ColumnReaderImpl.<init>(ColumnReaderImpl.java:46)\r\n\tat org.apache.parquet.column.impl.ColumnReadStoreImpl.getColumnReader(ColumnReadStoreImpl.java:82)\r\n\tat org.apache.parquet.io.RecordReaderImplementation.<init>(RecordReaderImplementation.java:271)\r\n\tat org.apache.parquet.io.MessageColumnIO$1.visit(MessageColumnIO.java:147)\r\n\tat org.apache.parquet.io.MessageColumnIO$1.visit(MessageColumnIO.java:109)\r\n\tat org.apache.parquet.filter2.compat.FilterCompat$NoOpFilter.accept(FilterCompat.java:177)\r\n\tat org.apache.parquet.io.MessageColumnIO.getRecordReader(MessageColumnIO.java:109)\r\n\tat org.apache.parquet.hadoop.InternalParquetRecordReader.checkRead(InternalParquetRecordReader.java:136)\r\n\tat org.apache.parquet.hadoop.InternalParquetRecordReader.nextKeyValue(InternalParquetRecordReader.java:225)\r\n\t... 17 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mPy4JJavaError\u001B[0m                             Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp\\ipykernel_19988\\1682671611.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[1;32m----> 1\u001B[1;33m \u001B[0mdf_bet\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mwrite\u001B[0m\u001B[0;31m \u001B[0m\u001B[0;31m\\\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      2\u001B[0m     \u001B[1;33m.\u001B[0m\u001B[0mmode\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mconfig\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mwrite_mode\u001B[0m\u001B[1;33m)\u001B[0m\u001B[0;31m \u001B[0m\u001B[0;31m\\\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      3\u001B[0m     \u001B[1;33m.\u001B[0m\u001B[0mformat\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mconfig\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mwrite_format\u001B[0m\u001B[1;33m)\u001B[0m\u001B[0;31m\\\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      4\u001B[0m     \u001B[1;33m.\u001B[0m\u001B[0msaveAsTable\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m\"c2c_bet\"\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      5\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\app\\python38\\lib\\site-packages\\pyspark\\sql\\readwriter.py\u001B[0m in \u001B[0;36msaveAsTable\u001B[1;34m(self, name, format, mode, partitionBy, **options)\u001B[0m\n\u001B[0;32m    804\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[0mformat\u001B[0m \u001B[1;32mis\u001B[0m \u001B[1;32mnot\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    805\u001B[0m             \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mformat\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mformat\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 806\u001B[1;33m         \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_jwrite\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msaveAsTable\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mname\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    807\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    808\u001B[0m     def json(self, path, mode=None, compression=None, dateFormat=None, timestampFormat=None,\n",
      "\u001B[1;32mD:\\app\\python38\\lib\\site-packages\\py4j\\java_gateway.py\u001B[0m in \u001B[0;36m__call__\u001B[1;34m(self, *args)\u001B[0m\n\u001B[0;32m   1319\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1320\u001B[0m         \u001B[0manswer\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mgateway_client\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msend_command\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mcommand\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1321\u001B[1;33m         return_value = get_return_value(\n\u001B[0m\u001B[0;32m   1322\u001B[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001B[0;32m   1323\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\app\\python38\\lib\\site-packages\\pyspark\\sql\\utils.py\u001B[0m in \u001B[0;36mdeco\u001B[1;34m(*a, **kw)\u001B[0m\n\u001B[0;32m    109\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0mdeco\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0ma\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkw\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    110\u001B[0m         \u001B[1;32mtry\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 111\u001B[1;33m             \u001B[1;32mreturn\u001B[0m \u001B[0mf\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0ma\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkw\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    112\u001B[0m         \u001B[1;32mexcept\u001B[0m \u001B[0mpy4j\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mprotocol\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mPy4JJavaError\u001B[0m \u001B[1;32mas\u001B[0m \u001B[0me\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    113\u001B[0m             \u001B[0mconverted\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mconvert_exception\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0me\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mjava_exception\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\app\\python38\\lib\\site-packages\\py4j\\protocol.py\u001B[0m in \u001B[0;36mget_return_value\u001B[1;34m(answer, gateway_client, target_id, name)\u001B[0m\n\u001B[0;32m    324\u001B[0m             \u001B[0mvalue\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mOUTPUT_CONVERTER\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mtype\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0manswer\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m2\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mgateway_client\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    325\u001B[0m             \u001B[1;32mif\u001B[0m \u001B[0manswer\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m]\u001B[0m \u001B[1;33m==\u001B[0m \u001B[0mREFERENCE_TYPE\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 326\u001B[1;33m                 raise Py4JJavaError(\n\u001B[0m\u001B[0;32m    327\u001B[0m                     \u001B[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001B[0m\u001B[1;33m.\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    328\u001B[0m                     format(target_id, \".\", name), value)\n",
      "\u001B[1;31mPy4JJavaError\u001B[0m: An error occurred while calling o469.saveAsTable.\n: org.apache.spark.SparkException: Job aborted.\r\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.jobAbortedError(QueryExecutionErrors.scala:496)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:251)\r\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:186)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource.writeAndRead(DataSource.scala:537)\r\n\tat org.apache.spark.sql.execution.command.CreateDataSourceTableAsSelectCommand.saveDataIntoTable(createDataSourceTables.scala:228)\r\n\tat org.apache.spark.sql.execution.command.CreateDataSourceTableAsSelectCommand.run(createDataSourceTables.scala:182)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:110)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:110)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:106)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:481)\r\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:82)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:481)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:457)\r\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:106)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:93)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:91)\r\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:128)\r\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:848)\r\n\tat org.apache.spark.sql.DataFrameWriter.createTable(DataFrameWriter.scala:689)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveAsTable(DataFrameWriter.scala:667)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveAsTable(DataFrameWriter.scala:565)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:564)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:832)\r\nCaused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 7 in stage 31.0 failed 1 times, most recent failure: Lost task 7.0 in stage 31.0 (TID 143) (host.docker.internal executor driver): org.apache.spark.sql.execution.QueryExecutionException: Encounter error while reading parquet files. One possible cause: Parquet column cannot be converted in the corresponding files. Details: \r\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.cannotReadParquetFilesError(QueryExecutionErrors.scala:577)\r\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:200)\r\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:104)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:286)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$16(FileFormatWriter.scala:229)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630)\r\n\tat java.base/java.lang.Thread.run(Thread.java:832)\r\nCaused by: org.apache.parquet.io.ParquetDecodingException: Can not read value at 0 in block -1 in file file:/D:/workspace/digipay-data-science/c2c-bet-fraud/c2cbet/c2c_bet/part-00003-c295f7b9-63fd-4f7d-b3da-9fb623fa77cc-c000.snappy.parquet\r\n\tat org.apache.parquet.hadoop.InternalParquetRecordReader.nextKeyValue(InternalParquetRecordReader.java:254)\r\n\tat org.apache.parquet.hadoop.ParquetRecordReader.nextKeyValue(ParquetRecordReader.java:207)\r\n\tat org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)\r\n\tat org.apache.spark.sql.execution.datasources.RecordReaderIterator$$anon$1.hasNext(RecordReaderIterator.scala:61)\r\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:104)\r\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:191)\r\n\t... 12 more\r\nCaused by: java.lang.UnsupportedOperationException: org.apache.parquet.column.values.dictionary.PlainValuesDictionary$PlainDoubleDictionary\r\n\tat org.apache.parquet.column.Dictionary.decodeToBinary(Dictionary.java:41)\r\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetRowConverter$ParquetStringConverter.$anonfun$setDictionary$1(ParquetRowConverter.scala:449)\r\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetRowConverter$ParquetStringConverter.$anonfun$setDictionary$1$adapted(ParquetRowConverter.scala:448)\r\n\tat scala.Array$.tabulate(Array.scala:418)\r\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetRowConverter$ParquetStringConverter.setDictionary(ParquetRowConverter.scala:448)\r\n\tat org.apache.parquet.column.impl.ColumnReaderBase.<init>(ColumnReaderBase.java:415)\r\n\tat org.apache.parquet.column.impl.ColumnReaderImpl.<init>(ColumnReaderImpl.java:46)\r\n\tat org.apache.parquet.column.impl.ColumnReadStoreImpl.getColumnReader(ColumnReadStoreImpl.java:82)\r\n\tat org.apache.parquet.io.RecordReaderImplementation.<init>(RecordReaderImplementation.java:271)\r\n\tat org.apache.parquet.io.MessageColumnIO$1.visit(MessageColumnIO.java:147)\r\n\tat org.apache.parquet.io.MessageColumnIO$1.visit(MessageColumnIO.java:109)\r\n\tat org.apache.parquet.filter2.compat.FilterCompat$NoOpFilter.accept(FilterCompat.java:177)\r\n\tat org.apache.parquet.io.MessageColumnIO.getRecordReader(MessageColumnIO.java:109)\r\n\tat org.apache.parquet.hadoop.InternalParquetRecordReader.checkRead(InternalParquetRecordReader.java:136)\r\n\tat org.apache.parquet.hadoop.InternalParquetRecordReader.nextKeyValue(InternalParquetRecordReader.java:225)\r\n\t... 17 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2454)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2403)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2402)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2402)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1160)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1160)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1160)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2642)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2584)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2573)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:938)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2214)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:218)\r\n\t... 44 more\r\nCaused by: org.apache.spark.sql.execution.QueryExecutionException: Encounter error while reading parquet files. One possible cause: Parquet column cannot be converted in the corresponding files. Details: \r\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.cannotReadParquetFilesError(QueryExecutionErrors.scala:577)\r\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:200)\r\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:104)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:286)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$16(FileFormatWriter.scala:229)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630)\r\n\t... 1 more\r\nCaused by: org.apache.parquet.io.ParquetDecodingException: Can not read value at 0 in block -1 in file file:/D:/workspace/digipay-data-science/c2c-bet-fraud/c2cbet/c2c_bet/part-00003-c295f7b9-63fd-4f7d-b3da-9fb623fa77cc-c000.snappy.parquet\r\n\tat org.apache.parquet.hadoop.InternalParquetRecordReader.nextKeyValue(InternalParquetRecordReader.java:254)\r\n\tat org.apache.parquet.hadoop.ParquetRecordReader.nextKeyValue(ParquetRecordReader.java:207)\r\n\tat org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)\r\n\tat org.apache.spark.sql.execution.datasources.RecordReaderIterator$$anon$1.hasNext(RecordReaderIterator.scala:61)\r\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:104)\r\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:191)\r\n\t... 12 more\r\nCaused by: java.lang.UnsupportedOperationException: org.apache.parquet.column.values.dictionary.PlainValuesDictionary$PlainDoubleDictionary\r\n\tat org.apache.parquet.column.Dictionary.decodeToBinary(Dictionary.java:41)\r\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetRowConverter$ParquetStringConverter.$anonfun$setDictionary$1(ParquetRowConverter.scala:449)\r\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetRowConverter$ParquetStringConverter.$anonfun$setDictionary$1$adapted(ParquetRowConverter.scala:448)\r\n\tat scala.Array$.tabulate(Array.scala:418)\r\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetRowConverter$ParquetStringConverter.setDictionary(ParquetRowConverter.scala:448)\r\n\tat org.apache.parquet.column.impl.ColumnReaderBase.<init>(ColumnReaderBase.java:415)\r\n\tat org.apache.parquet.column.impl.ColumnReaderImpl.<init>(ColumnReaderImpl.java:46)\r\n\tat org.apache.parquet.column.impl.ColumnReadStoreImpl.getColumnReader(ColumnReadStoreImpl.java:82)\r\n\tat org.apache.parquet.io.RecordReaderImplementation.<init>(RecordReaderImplementation.java:271)\r\n\tat org.apache.parquet.io.MessageColumnIO$1.visit(MessageColumnIO.java:147)\r\n\tat org.apache.parquet.io.MessageColumnIO$1.visit(MessageColumnIO.java:109)\r\n\tat org.apache.parquet.filter2.compat.FilterCompat$NoOpFilter.accept(FilterCompat.java:177)\r\n\tat org.apache.parquet.io.MessageColumnIO.getRecordReader(MessageColumnIO.java:109)\r\n\tat org.apache.parquet.hadoop.InternalParquetRecordReader.checkRead(InternalParquetRecordReader.java:136)\r\n\tat org.apache.parquet.hadoop.InternalParquetRecordReader.nextKeyValue(InternalParquetRecordReader.java:225)\r\n\t... 17 more\r\n"
     ]
    }
   ],
   "source": [
    "df_bet.write \\\n",
    "    .mode(config.write_mode) \\\n",
    "    .format(config.write_format)\\\n",
    "    .saveAsTable(\"c2c_bet\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [
    {
     "data": {
      "text/plain": "DataFrame[]"
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"DROP TABLE IF EXISTS c2c_bet\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o23.sql.\n: org.apache.spark.SparkException: Job aborted.\r\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.jobAbortedError(QueryExecutionErrors.scala:496)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:251)\r\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:186)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource.writeAndRead(DataSource.scala:537)\r\n\tat org.apache.spark.sql.execution.command.CreateDataSourceTableAsSelectCommand.saveDataIntoTable(createDataSourceTables.scala:228)\r\n\tat org.apache.spark.sql.execution.command.CreateDataSourceTableAsSelectCommand.run(createDataSourceTables.scala:182)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:110)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:110)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:106)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:481)\r\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:82)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:481)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:457)\r\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:106)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:93)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:91)\r\n\tat org.apache.spark.sql.Dataset.<init>(Dataset.scala:219)\r\n\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\r\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:96)\r\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:618)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\r\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:613)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:564)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:832)\r\nCaused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 3 in stage 16.0 failed 1 times, most recent failure: Lost task 3.0 in stage 16.0 (TID 33) (host.docker.internal executor driver): org.apache.spark.sql.execution.QueryExecutionException: Parquet column cannot be converted in file file:///D:/workspace/digipay-data-science/c2c-bet-fraud/c2cbet/c2c_bet/part-00004-d84ce583-95f1-4ca2-9c10-e3173749b71c-c000.snappy.parquet. Column: [channel], Expected: string, Found: DOUBLE\r\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.unsupportedSchemaColumnConvertError(QueryExecutionErrors.scala:570)\r\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:195)\r\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:104)\r\n\tat org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:522)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:286)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$16(FileFormatWriter.scala:229)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630)\r\n\tat java.base/java.lang.Thread.run(Thread.java:832)\r\nCaused by: org.apache.spark.sql.execution.datasources.SchemaColumnConvertNotSupportedException\r\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.constructConvertNotSupportedException(ParquetVectorUpdaterFactory.java:1104)\r\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.getUpdater(ParquetVectorUpdaterFactory.java:181)\r\n\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedColumnReader.readBatch(VectorizedColumnReader.java:161)\r\n\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextBatch(VectorizedParquetRecordReader.java:298)\r\n\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextKeyValue(VectorizedParquetRecordReader.java:196)\r\n\tat org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)\r\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:104)\r\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:191)\r\n\t... 16 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2454)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2403)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2402)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2402)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1160)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1160)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1160)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2642)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2584)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2573)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:938)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2214)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:218)\r\n\t... 46 more\r\nCaused by: org.apache.spark.sql.execution.QueryExecutionException: Parquet column cannot be converted in file file:///D:/workspace/digipay-data-science/c2c-bet-fraud/c2cbet/c2c_bet/part-00004-d84ce583-95f1-4ca2-9c10-e3173749b71c-c000.snappy.parquet. Column: [channel], Expected: string, Found: DOUBLE\r\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.unsupportedSchemaColumnConvertError(QueryExecutionErrors.scala:570)\r\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:195)\r\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:104)\r\n\tat org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:522)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:286)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$16(FileFormatWriter.scala:229)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630)\r\n\t... 1 more\r\nCaused by: org.apache.spark.sql.execution.datasources.SchemaColumnConvertNotSupportedException\r\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.constructConvertNotSupportedException(ParquetVectorUpdaterFactory.java:1104)\r\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.getUpdater(ParquetVectorUpdaterFactory.java:181)\r\n\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedColumnReader.readBatch(VectorizedColumnReader.java:161)\r\n\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextBatch(VectorizedParquetRecordReader.java:298)\r\n\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextKeyValue(VectorizedParquetRecordReader.java:196)\r\n\tat org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)\r\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:104)\r\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:191)\r\n\t... 16 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mPy4JJavaError\u001B[0m                             Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp\\ipykernel_19988\\2002836327.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[1;32m----> 1\u001B[1;33m spark.sql(\"CREATE TABLE c2c_bet using parquet \"\n\u001B[0m\u001B[0;32m      2\u001B[0m           \" AS SELECT * FROM parquet.c2c_bet\")\n\u001B[0;32m      3\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\app\\python38\\lib\\site-packages\\pyspark\\sql\\session.py\u001B[0m in \u001B[0;36msql\u001B[1;34m(self, sqlQuery)\u001B[0m\n\u001B[0;32m    721\u001B[0m         \u001B[1;33m[\u001B[0m\u001B[0mRow\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mf1\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mf2\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;34m'row1'\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mRow\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mf1\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;36m2\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mf2\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;34m'row2'\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mRow\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mf1\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;36m3\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mf2\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;34m'row3'\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    722\u001B[0m         \"\"\"\n\u001B[1;32m--> 723\u001B[1;33m         \u001B[1;32mreturn\u001B[0m \u001B[0mDataFrame\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_jsparkSession\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msql\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0msqlQuery\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_wrapped\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    724\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    725\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0mtable\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtableName\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\app\\python38\\lib\\site-packages\\py4j\\java_gateway.py\u001B[0m in \u001B[0;36m__call__\u001B[1;34m(self, *args)\u001B[0m\n\u001B[0;32m   1319\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1320\u001B[0m         \u001B[0manswer\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mgateway_client\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msend_command\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mcommand\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1321\u001B[1;33m         return_value = get_return_value(\n\u001B[0m\u001B[0;32m   1322\u001B[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001B[0;32m   1323\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\app\\python38\\lib\\site-packages\\pyspark\\sql\\utils.py\u001B[0m in \u001B[0;36mdeco\u001B[1;34m(*a, **kw)\u001B[0m\n\u001B[0;32m    109\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0mdeco\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0ma\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkw\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    110\u001B[0m         \u001B[1;32mtry\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 111\u001B[1;33m             \u001B[1;32mreturn\u001B[0m \u001B[0mf\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0ma\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkw\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    112\u001B[0m         \u001B[1;32mexcept\u001B[0m \u001B[0mpy4j\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mprotocol\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mPy4JJavaError\u001B[0m \u001B[1;32mas\u001B[0m \u001B[0me\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    113\u001B[0m             \u001B[0mconverted\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mconvert_exception\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0me\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mjava_exception\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\app\\python38\\lib\\site-packages\\py4j\\protocol.py\u001B[0m in \u001B[0;36mget_return_value\u001B[1;34m(answer, gateway_client, target_id, name)\u001B[0m\n\u001B[0;32m    324\u001B[0m             \u001B[0mvalue\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mOUTPUT_CONVERTER\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mtype\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0manswer\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m2\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mgateway_client\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    325\u001B[0m             \u001B[1;32mif\u001B[0m \u001B[0manswer\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m]\u001B[0m \u001B[1;33m==\u001B[0m \u001B[0mREFERENCE_TYPE\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 326\u001B[1;33m                 raise Py4JJavaError(\n\u001B[0m\u001B[0;32m    327\u001B[0m                     \u001B[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001B[0m\u001B[1;33m.\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    328\u001B[0m                     format(target_id, \".\", name), value)\n",
      "\u001B[1;31mPy4JJavaError\u001B[0m: An error occurred while calling o23.sql.\n: org.apache.spark.SparkException: Job aborted.\r\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.jobAbortedError(QueryExecutionErrors.scala:496)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:251)\r\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:186)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource.writeAndRead(DataSource.scala:537)\r\n\tat org.apache.spark.sql.execution.command.CreateDataSourceTableAsSelectCommand.saveDataIntoTable(createDataSourceTables.scala:228)\r\n\tat org.apache.spark.sql.execution.command.CreateDataSourceTableAsSelectCommand.run(createDataSourceTables.scala:182)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:110)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:110)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:106)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:481)\r\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:82)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:481)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:457)\r\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:106)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:93)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:91)\r\n\tat org.apache.spark.sql.Dataset.<init>(Dataset.scala:219)\r\n\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\r\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:96)\r\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:618)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\r\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:613)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:564)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:832)\r\nCaused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 3 in stage 16.0 failed 1 times, most recent failure: Lost task 3.0 in stage 16.0 (TID 33) (host.docker.internal executor driver): org.apache.spark.sql.execution.QueryExecutionException: Parquet column cannot be converted in file file:///D:/workspace/digipay-data-science/c2c-bet-fraud/c2cbet/c2c_bet/part-00004-d84ce583-95f1-4ca2-9c10-e3173749b71c-c000.snappy.parquet. Column: [channel], Expected: string, Found: DOUBLE\r\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.unsupportedSchemaColumnConvertError(QueryExecutionErrors.scala:570)\r\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:195)\r\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:104)\r\n\tat org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:522)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:286)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$16(FileFormatWriter.scala:229)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630)\r\n\tat java.base/java.lang.Thread.run(Thread.java:832)\r\nCaused by: org.apache.spark.sql.execution.datasources.SchemaColumnConvertNotSupportedException\r\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.constructConvertNotSupportedException(ParquetVectorUpdaterFactory.java:1104)\r\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.getUpdater(ParquetVectorUpdaterFactory.java:181)\r\n\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedColumnReader.readBatch(VectorizedColumnReader.java:161)\r\n\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextBatch(VectorizedParquetRecordReader.java:298)\r\n\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextKeyValue(VectorizedParquetRecordReader.java:196)\r\n\tat org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)\r\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:104)\r\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:191)\r\n\t... 16 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2454)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2403)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2402)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2402)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1160)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1160)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1160)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2642)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2584)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2573)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:938)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2214)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:218)\r\n\t... 46 more\r\nCaused by: org.apache.spark.sql.execution.QueryExecutionException: Parquet column cannot be converted in file file:///D:/workspace/digipay-data-science/c2c-bet-fraud/c2cbet/c2c_bet/part-00004-d84ce583-95f1-4ca2-9c10-e3173749b71c-c000.snappy.parquet. Column: [channel], Expected: string, Found: DOUBLE\r\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.unsupportedSchemaColumnConvertError(QueryExecutionErrors.scala:570)\r\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:195)\r\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:104)\r\n\tat org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:522)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:286)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$16(FileFormatWriter.scala:229)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630)\r\n\t... 1 more\r\nCaused by: org.apache.spark.sql.execution.datasources.SchemaColumnConvertNotSupportedException\r\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.constructConvertNotSupportedException(ParquetVectorUpdaterFactory.java:1104)\r\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.getUpdater(ParquetVectorUpdaterFactory.java:181)\r\n\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedColumnReader.readBatch(VectorizedColumnReader.java:161)\r\n\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextBatch(VectorizedParquetRecordReader.java:298)\r\n\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextKeyValue(VectorizedParquetRecordReader.java:196)\r\n\tat org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)\r\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:104)\r\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:191)\r\n\t... 16 more\r\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"CREATE TABLE c2c_bet using parquet \"\n",
    "          \" AS SELECT * FROM parquet.c2c_bet\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "data": {
      "text/plain": "<pyspark.sql.session.SparkSession at 0x19e4abdd9a0>",
      "text/html": "\n            <div>\n                <p><b>SparkSession - in-memory</b></p>\n                \n        <div>\n            <p><b>SparkContext</b></p>\n\n            <p><a href=\"http://host.docker.internal:4040\">Spark UI</a></p>\n\n            <dl>\n              <dt>Version</dt>\n                <dd><code>v3.2.1</code></dd>\n              <dt>Master</dt>\n                <dd><code>local[*]</code></dd>\n              <dt>AppName</dt>\n                <dd><code>DP-c2c-bet-fraud-detection</code></dd>\n            </dl>\n        </div>\n        \n            </div>\n        "
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hlpr.recreate_spark()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o87.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 22.0 failed 1 times, most recent failure: Lost task 1.0 in stage 22.0 (TID 38) (host.docker.internal executor driver): org.apache.spark.sql.execution.QueryExecutionException: Parquet column cannot be converted in file file:///D:/workspace/digipay-data-science/c2c-bet-fraud/c2cbet/c2c_bet/part-00007-d84ce583-95f1-4ca2-9c10-e3173749b71c-c000.snappy.parquet. Column: [channel], Expected: string, Found: DOUBLE\r\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.unsupportedSchemaColumnConvertError(QueryExecutionErrors.scala:570)\r\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:195)\r\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:104)\r\n\tat org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:522)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:349)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:898)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:898)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630)\r\n\tat java.base/java.lang.Thread.run(Thread.java:832)\r\nCaused by: org.apache.spark.sql.execution.datasources.SchemaColumnConvertNotSupportedException\r\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.constructConvertNotSupportedException(ParquetVectorUpdaterFactory.java:1104)\r\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.getUpdater(ParquetVectorUpdaterFactory.java:181)\r\n\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedColumnReader.readBatch(VectorizedColumnReader.java:161)\r\n\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextBatch(VectorizedParquetRecordReader.java:298)\r\n\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextKeyValue(VectorizedParquetRecordReader.java:196)\r\n\tat org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)\r\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:104)\r\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:191)\r\n\t... 20 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2454)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2403)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2402)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2402)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1160)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1160)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1160)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2642)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2584)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2573)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:938)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2214)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2235)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2254)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:476)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:429)\r\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:48)\r\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3715)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:2728)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3706)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\r\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3704)\r\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2728)\r\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2935)\r\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:287)\r\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:326)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:564)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:832)\r\nCaused by: org.apache.spark.sql.execution.QueryExecutionException: Parquet column cannot be converted in file file:///D:/workspace/digipay-data-science/c2c-bet-fraud/c2cbet/c2c_bet/part-00007-d84ce583-95f1-4ca2-9c10-e3173749b71c-c000.snappy.parquet. Column: [channel], Expected: string, Found: DOUBLE\r\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.unsupportedSchemaColumnConvertError(QueryExecutionErrors.scala:570)\r\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:195)\r\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:104)\r\n\tat org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:522)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:349)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:898)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:898)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630)\r\n\t... 1 more\r\nCaused by: org.apache.spark.sql.execution.datasources.SchemaColumnConvertNotSupportedException\r\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.constructConvertNotSupportedException(ParquetVectorUpdaterFactory.java:1104)\r\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.getUpdater(ParquetVectorUpdaterFactory.java:181)\r\n\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedColumnReader.readBatch(VectorizedColumnReader.java:161)\r\n\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextBatch(VectorizedParquetRecordReader.java:298)\r\n\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextKeyValue(VectorizedParquetRecordReader.java:196)\r\n\tat org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)\r\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:104)\r\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:191)\r\n\t... 20 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mPy4JJavaError\u001B[0m                             Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp\\ipykernel_10176\\2396139368.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[1;32m----> 1\u001B[1;33m \u001B[0mspark\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msql\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m\"select channel from parquet.c2c_bet\"\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mshow\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;36m300\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[1;32mD:\\app\\python38\\lib\\site-packages\\pyspark\\sql\\dataframe.py\u001B[0m in \u001B[0;36mshow\u001B[1;34m(self, n, truncate, vertical)\u001B[0m\n\u001B[0;32m    492\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    493\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[0misinstance\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtruncate\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mbool\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;32mand\u001B[0m \u001B[0mtruncate\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 494\u001B[1;33m             \u001B[0mprint\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_jdf\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mshowString\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mn\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;36m20\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mvertical\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    495\u001B[0m         \u001B[1;32melse\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    496\u001B[0m             \u001B[1;32mtry\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\app\\python38\\lib\\site-packages\\py4j\\java_gateway.py\u001B[0m in \u001B[0;36m__call__\u001B[1;34m(self, *args)\u001B[0m\n\u001B[0;32m   1319\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1320\u001B[0m         \u001B[0manswer\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mgateway_client\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msend_command\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mcommand\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1321\u001B[1;33m         return_value = get_return_value(\n\u001B[0m\u001B[0;32m   1322\u001B[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001B[0;32m   1323\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\app\\python38\\lib\\site-packages\\pyspark\\sql\\utils.py\u001B[0m in \u001B[0;36mdeco\u001B[1;34m(*a, **kw)\u001B[0m\n\u001B[0;32m    109\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0mdeco\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0ma\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkw\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    110\u001B[0m         \u001B[1;32mtry\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 111\u001B[1;33m             \u001B[1;32mreturn\u001B[0m \u001B[0mf\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0ma\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkw\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    112\u001B[0m         \u001B[1;32mexcept\u001B[0m \u001B[0mpy4j\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mprotocol\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mPy4JJavaError\u001B[0m \u001B[1;32mas\u001B[0m \u001B[0me\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    113\u001B[0m             \u001B[0mconverted\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mconvert_exception\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0me\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mjava_exception\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\app\\python38\\lib\\site-packages\\py4j\\protocol.py\u001B[0m in \u001B[0;36mget_return_value\u001B[1;34m(answer, gateway_client, target_id, name)\u001B[0m\n\u001B[0;32m    324\u001B[0m             \u001B[0mvalue\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mOUTPUT_CONVERTER\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mtype\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0manswer\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m2\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mgateway_client\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    325\u001B[0m             \u001B[1;32mif\u001B[0m \u001B[0manswer\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m]\u001B[0m \u001B[1;33m==\u001B[0m \u001B[0mREFERENCE_TYPE\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 326\u001B[1;33m                 raise Py4JJavaError(\n\u001B[0m\u001B[0;32m    327\u001B[0m                     \u001B[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001B[0m\u001B[1;33m.\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    328\u001B[0m                     format(target_id, \".\", name), value)\n",
      "\u001B[1;31mPy4JJavaError\u001B[0m: An error occurred while calling o87.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 22.0 failed 1 times, most recent failure: Lost task 1.0 in stage 22.0 (TID 38) (host.docker.internal executor driver): org.apache.spark.sql.execution.QueryExecutionException: Parquet column cannot be converted in file file:///D:/workspace/digipay-data-science/c2c-bet-fraud/c2cbet/c2c_bet/part-00007-d84ce583-95f1-4ca2-9c10-e3173749b71c-c000.snappy.parquet. Column: [channel], Expected: string, Found: DOUBLE\r\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.unsupportedSchemaColumnConvertError(QueryExecutionErrors.scala:570)\r\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:195)\r\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:104)\r\n\tat org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:522)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:349)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:898)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:898)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630)\r\n\tat java.base/java.lang.Thread.run(Thread.java:832)\r\nCaused by: org.apache.spark.sql.execution.datasources.SchemaColumnConvertNotSupportedException\r\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.constructConvertNotSupportedException(ParquetVectorUpdaterFactory.java:1104)\r\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.getUpdater(ParquetVectorUpdaterFactory.java:181)\r\n\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedColumnReader.readBatch(VectorizedColumnReader.java:161)\r\n\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextBatch(VectorizedParquetRecordReader.java:298)\r\n\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextKeyValue(VectorizedParquetRecordReader.java:196)\r\n\tat org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)\r\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:104)\r\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:191)\r\n\t... 20 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2454)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2403)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2402)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2402)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1160)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1160)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1160)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2642)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2584)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2573)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:938)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2214)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2235)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2254)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:476)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:429)\r\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:48)\r\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3715)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:2728)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3706)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\r\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3704)\r\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2728)\r\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2935)\r\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:287)\r\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:326)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:564)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:832)\r\nCaused by: org.apache.spark.sql.execution.QueryExecutionException: Parquet column cannot be converted in file file:///D:/workspace/digipay-data-science/c2c-bet-fraud/c2cbet/c2c_bet/part-00007-d84ce583-95f1-4ca2-9c10-e3173749b71c-c000.snappy.parquet. Column: [channel], Expected: string, Found: DOUBLE\r\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.unsupportedSchemaColumnConvertError(QueryExecutionErrors.scala:570)\r\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:195)\r\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:104)\r\n\tat org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:522)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:349)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:898)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:898)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630)\r\n\t... 1 more\r\nCaused by: org.apache.spark.sql.execution.datasources.SchemaColumnConvertNotSupportedException\r\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.constructConvertNotSupportedException(ParquetVectorUpdaterFactory.java:1104)\r\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.getUpdater(ParquetVectorUpdaterFactory.java:181)\r\n\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedColumnReader.readBatch(VectorizedColumnReader.java:161)\r\n\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextBatch(VectorizedParquetRecordReader.java:298)\r\n\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextKeyValue(VectorizedParquetRecordReader.java:196)\r\n\tat org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)\r\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:104)\r\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:191)\r\n\t... 20 more\r\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select channel from parquet.c2c_bet\").show(300)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o23.sql.\n: org.apache.spark.SparkException: Job aborted.\r\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.jobAbortedError(QueryExecutionErrors.scala:496)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:251)\r\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:186)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource.writeAndRead(DataSource.scala:537)\r\n\tat org.apache.spark.sql.execution.command.CreateDataSourceTableAsSelectCommand.saveDataIntoTable(createDataSourceTables.scala:228)\r\n\tat org.apache.spark.sql.execution.command.CreateDataSourceTableAsSelectCommand.run(createDataSourceTables.scala:182)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:110)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:110)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:106)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:481)\r\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:82)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:481)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:457)\r\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:106)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:93)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:91)\r\n\tat org.apache.spark.sql.Dataset.<init>(Dataset.scala:219)\r\n\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\r\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:96)\r\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:618)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\r\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:613)\r\n\tat jdk.internal.reflect.GeneratedMethodAccessor58.invoke(Unknown Source)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:564)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:832)\r\nCaused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 4 in stage 26.0 failed 1 times, most recent failure: Lost task 4.0 in stage 26.0 (TID 46) (host.docker.internal executor driver): org.apache.spark.sql.execution.QueryExecutionException: Parquet column cannot be converted in file file:///D:/workspace/digipay-data-science/c2c-bet-fraud/c2cbet/c2c_bet/part-00002-d84ce583-95f1-4ca2-9c10-e3173749b71c-c000.snappy.parquet. Column: [channel], Expected: string, Found: DOUBLE\r\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.unsupportedSchemaColumnConvertError(QueryExecutionErrors.scala:570)\r\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:195)\r\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:104)\r\n\tat org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:522)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:286)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$16(FileFormatWriter.scala:229)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630)\r\n\tat java.base/java.lang.Thread.run(Thread.java:832)\r\nCaused by: org.apache.spark.sql.execution.datasources.SchemaColumnConvertNotSupportedException\r\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.constructConvertNotSupportedException(ParquetVectorUpdaterFactory.java:1104)\r\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.getUpdater(ParquetVectorUpdaterFactory.java:181)\r\n\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedColumnReader.readBatch(VectorizedColumnReader.java:161)\r\n\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextBatch(VectorizedParquetRecordReader.java:298)\r\n\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextKeyValue(VectorizedParquetRecordReader.java:196)\r\n\tat org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)\r\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:104)\r\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:191)\r\n\t... 16 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2454)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2403)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2402)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2402)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1160)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1160)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1160)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2642)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2584)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2573)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:938)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2214)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:218)\r\n\t... 45 more\r\nCaused by: org.apache.spark.sql.execution.QueryExecutionException: Parquet column cannot be converted in file file:///D:/workspace/digipay-data-science/c2c-bet-fraud/c2cbet/c2c_bet/part-00002-d84ce583-95f1-4ca2-9c10-e3173749b71c-c000.snappy.parquet. Column: [channel], Expected: string, Found: DOUBLE\r\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.unsupportedSchemaColumnConvertError(QueryExecutionErrors.scala:570)\r\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:195)\r\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:104)\r\n\tat org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:522)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:286)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$16(FileFormatWriter.scala:229)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630)\r\n\t... 1 more\r\nCaused by: org.apache.spark.sql.execution.datasources.SchemaColumnConvertNotSupportedException\r\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.constructConvertNotSupportedException(ParquetVectorUpdaterFactory.java:1104)\r\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.getUpdater(ParquetVectorUpdaterFactory.java:181)\r\n\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedColumnReader.readBatch(VectorizedColumnReader.java:161)\r\n\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextBatch(VectorizedParquetRecordReader.java:298)\r\n\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextKeyValue(VectorizedParquetRecordReader.java:196)\r\n\tat org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)\r\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:104)\r\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:191)\r\n\t... 16 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mPy4JJavaError\u001B[0m                             Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp\\ipykernel_10176\\1704521310.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[1;32m----> 1\u001B[1;33m \u001B[0mspark\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msql\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m\"create table c2c_bet2 using parquet as select * from parquet.c2c_bet\"\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[1;32mD:\\app\\python38\\lib\\site-packages\\pyspark\\sql\\session.py\u001B[0m in \u001B[0;36msql\u001B[1;34m(self, sqlQuery)\u001B[0m\n\u001B[0;32m    721\u001B[0m         \u001B[1;33m[\u001B[0m\u001B[0mRow\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mf1\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mf2\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;34m'row1'\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mRow\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mf1\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;36m2\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mf2\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;34m'row2'\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mRow\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mf1\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;36m3\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mf2\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;34m'row3'\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    722\u001B[0m         \"\"\"\n\u001B[1;32m--> 723\u001B[1;33m         \u001B[1;32mreturn\u001B[0m \u001B[0mDataFrame\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_jsparkSession\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msql\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0msqlQuery\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_wrapped\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    724\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    725\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0mtable\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtableName\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\app\\python38\\lib\\site-packages\\py4j\\java_gateway.py\u001B[0m in \u001B[0;36m__call__\u001B[1;34m(self, *args)\u001B[0m\n\u001B[0;32m   1319\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1320\u001B[0m         \u001B[0manswer\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mgateway_client\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msend_command\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mcommand\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1321\u001B[1;33m         return_value = get_return_value(\n\u001B[0m\u001B[0;32m   1322\u001B[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001B[0;32m   1323\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\app\\python38\\lib\\site-packages\\pyspark\\sql\\utils.py\u001B[0m in \u001B[0;36mdeco\u001B[1;34m(*a, **kw)\u001B[0m\n\u001B[0;32m    109\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0mdeco\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0ma\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkw\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    110\u001B[0m         \u001B[1;32mtry\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 111\u001B[1;33m             \u001B[1;32mreturn\u001B[0m \u001B[0mf\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0ma\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkw\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    112\u001B[0m         \u001B[1;32mexcept\u001B[0m \u001B[0mpy4j\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mprotocol\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mPy4JJavaError\u001B[0m \u001B[1;32mas\u001B[0m \u001B[0me\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    113\u001B[0m             \u001B[0mconverted\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mconvert_exception\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0me\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mjava_exception\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\app\\python38\\lib\\site-packages\\py4j\\protocol.py\u001B[0m in \u001B[0;36mget_return_value\u001B[1;34m(answer, gateway_client, target_id, name)\u001B[0m\n\u001B[0;32m    324\u001B[0m             \u001B[0mvalue\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mOUTPUT_CONVERTER\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mtype\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0manswer\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m2\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mgateway_client\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    325\u001B[0m             \u001B[1;32mif\u001B[0m \u001B[0manswer\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m]\u001B[0m \u001B[1;33m==\u001B[0m \u001B[0mREFERENCE_TYPE\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 326\u001B[1;33m                 raise Py4JJavaError(\n\u001B[0m\u001B[0;32m    327\u001B[0m                     \u001B[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001B[0m\u001B[1;33m.\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    328\u001B[0m                     format(target_id, \".\", name), value)\n",
      "\u001B[1;31mPy4JJavaError\u001B[0m: An error occurred while calling o23.sql.\n: org.apache.spark.SparkException: Job aborted.\r\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.jobAbortedError(QueryExecutionErrors.scala:496)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:251)\r\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:186)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource.writeAndRead(DataSource.scala:537)\r\n\tat org.apache.spark.sql.execution.command.CreateDataSourceTableAsSelectCommand.saveDataIntoTable(createDataSourceTables.scala:228)\r\n\tat org.apache.spark.sql.execution.command.CreateDataSourceTableAsSelectCommand.run(createDataSourceTables.scala:182)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:110)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:110)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:106)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:481)\r\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:82)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:481)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:457)\r\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:106)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:93)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:91)\r\n\tat org.apache.spark.sql.Dataset.<init>(Dataset.scala:219)\r\n\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\r\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:96)\r\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:618)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\r\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:613)\r\n\tat jdk.internal.reflect.GeneratedMethodAccessor58.invoke(Unknown Source)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:564)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:832)\r\nCaused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 4 in stage 26.0 failed 1 times, most recent failure: Lost task 4.0 in stage 26.0 (TID 46) (host.docker.internal executor driver): org.apache.spark.sql.execution.QueryExecutionException: Parquet column cannot be converted in file file:///D:/workspace/digipay-data-science/c2c-bet-fraud/c2cbet/c2c_bet/part-00002-d84ce583-95f1-4ca2-9c10-e3173749b71c-c000.snappy.parquet. Column: [channel], Expected: string, Found: DOUBLE\r\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.unsupportedSchemaColumnConvertError(QueryExecutionErrors.scala:570)\r\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:195)\r\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:104)\r\n\tat org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:522)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:286)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$16(FileFormatWriter.scala:229)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630)\r\n\tat java.base/java.lang.Thread.run(Thread.java:832)\r\nCaused by: org.apache.spark.sql.execution.datasources.SchemaColumnConvertNotSupportedException\r\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.constructConvertNotSupportedException(ParquetVectorUpdaterFactory.java:1104)\r\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.getUpdater(ParquetVectorUpdaterFactory.java:181)\r\n\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedColumnReader.readBatch(VectorizedColumnReader.java:161)\r\n\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextBatch(VectorizedParquetRecordReader.java:298)\r\n\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextKeyValue(VectorizedParquetRecordReader.java:196)\r\n\tat org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)\r\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:104)\r\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:191)\r\n\t... 16 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2454)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2403)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2402)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2402)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1160)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1160)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1160)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2642)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2584)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2573)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:938)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2214)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:218)\r\n\t... 45 more\r\nCaused by: org.apache.spark.sql.execution.QueryExecutionException: Parquet column cannot be converted in file file:///D:/workspace/digipay-data-science/c2c-bet-fraud/c2cbet/c2c_bet/part-00002-d84ce583-95f1-4ca2-9c10-e3173749b71c-c000.snappy.parquet. Column: [channel], Expected: string, Found: DOUBLE\r\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.unsupportedSchemaColumnConvertError(QueryExecutionErrors.scala:570)\r\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:195)\r\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:104)\r\n\tat org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:522)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:286)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$16(FileFormatWriter.scala:229)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630)\r\n\t... 1 more\r\nCaused by: org.apache.spark.sql.execution.datasources.SchemaColumnConvertNotSupportedException\r\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.constructConvertNotSupportedException(ParquetVectorUpdaterFactory.java:1104)\r\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.getUpdater(ParquetVectorUpdaterFactory.java:181)\r\n\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedColumnReader.readBatch(VectorizedColumnReader.java:161)\r\n\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextBatch(VectorizedParquetRecordReader.java:298)\r\n\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextKeyValue(VectorizedParquetRecordReader.java:196)\r\n\tat org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)\r\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:104)\r\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:191)\r\n\t... 16 more\r\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"create table c2c_bet2 using parquet as select * from parquet.c2c_bet\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}